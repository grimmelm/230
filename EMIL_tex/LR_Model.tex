\newcommand{\xmax}{x_{\text{max}}}
\newcommand{\xbi}{x_{\text{BI}}^*}
\newcommand{\xsl}{x_{\text{SL}}^*}
\newcommand{\xsubi}{x_{\text{I}}^*}

\section{An Economic Model of Moderation}

There are two distinctive features of platform liability for harmful third-party content. The platform has \emph{imperfect information} about which content is harmful and which is not, and  content can have \emph{positive externalities} not captured by the platform itself, and  These two features, taken together, mean that holding the platform liable for the harmful content it carries can go wrong. Because the platform cannot perfectly distinguish harmful from harmless content, and because it does not internalize the full benefits from the harmless content, the thread of liability can cause the platform to overmoderate, removing too much harmless content along with the harmful content. To see how these effects arise, it is useful to start with a model in which they are absent.


\subsection{Harmful Content}

Imagine the content submitted by users to a platform arranged on a spectrum from worthwhile to worthless. At one end, the content is entertaining and informative -- cat pictures and civics lessons. At the other end, the content is stomach-churning or worse -- gross-out pictures and badly-written spam. A platform sets its moderation policy by deciding where along this spectrum to draw the line.

More formally, let the spectrum of content run from $0$ to $\xmax$ and let the marginal profit that the platform can make from hosting the $x$-th unit of content be $p(x)$. To capture the idea that the content is ordered from best to worst, assume that $p(x)$ is weakly decreasing -- i.e. is flat or slopes down as $x$ increases. Content for which $p(x) <0$ generates negative marginal profit -- that is, this content costs the platform more to host than it brings in. The platform chooses a moderation level $\hat{x}$. It hosts all content for which $x \le \hat{x}$ and takes down all content for which $x > \hat{x}$. As Figure \ref{fig:host1} illustrates, the platform's total profit will be the area under the curve $p(x)$ between $0$ and $\hat{x}$. 

\begin{pgfecon}{A platform chooses its moderation level}{fig:host1}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \dropline{4.28}{1.65}{$\hat{x}$}
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  % \addplot [pattern= north east lines, pattern color = red] fill between [of = welfare and lambda, soft clip={domain=4.28:5.4}];
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:4.28}];
\end{pgfecon}

In this situation, a rational platform will choose the value of $\hat{x}$ that maximizes its profits. As Figure \ref{fig:host2} illustrates, this occurs at the point $x^*$ where $p(x)$ crosses the $x$ axis and becomes negative. If the platform chooses $hat{x}$ to the left of $x^*$, each unit of content it adds will generate positive marginal profit, so it could make more money by hosting more content. But if the platform chooses $hat{x}$ to the right of $x^*$, the marginal unit of content generates negative marginal profit, so the platform can save money by hosting less content. Thus, $x^*$ is the profit-maximizing level of moderation. 

By the same reasoning, the point where $p(x)$ crosses the $x$ axis is also the socially efficient level of moderation. Because there are no externalities from hosting content, overall social welfare simply is the platform's profits. Thus, $x^e$, the socially efficient level of moderation, is equal to $x^*$, and the platform's incentives are aligned with society's.

\begin{pgfecon}{Without externalities, the platform has efficient incentives}{fig:host2}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \draw[dashed, thin] (8, 0) -- (8, -0.5) node[below]{$x^*=x^e$};
\end{pgfecon}

Regulation potentially becomes appropriate when the platform's incentives diverge from society's. The essential premise on which any form of liability depends is that some conduct is harmful. For a platform, that conduct is content. 

To model harmful content, we add a function $h(x)$ that measures how much harm the $x$-th unit of content causes to third parties. We assume that the same spectrum from that captures the platform's profits also describes the content's harmfulness.As one moves right along the $x$ axis, $p(x)$ falls and $h(x)$ rises. The platform's marginal profits are still $p(x)$; marginal social welfare is now $p(x) - h(x)$. To make the model interesting, we assume that the most innocuous content is completely harmless (i.e. $h(0) = 0$) and that the most problematic content is unambiguously harmful to society (i.e.  $p(\xmax) < h(\xmax)$). These conditions ensure that some content has positive social benefit and some content has negative social benefit, so that there is a real interest in treating them differently. It also follows, by the intermediate value theorem, that there is some point at which the curves $p(x)$ and $h(x)$ cross.

In Figure \ref{fig:harm1}, if the platform sets its moderation level at $\hat{x}$, then the green checked region beneath $p(x)$ corresponds to its profits and the red striped region beneath $h(x)$ corresponds to the harms caused by the content it carries. The efficient moderation level is at $x^e$, the point where they cross, where the marginal social welfare of one additional unit of content is zero. To the left of $x^e$, content is good because $p(x) > h(x)$; to the right of $x^e$, content is bad because $h(x) > p(x)$.

Figure \ref{fig:harm1} also illustrates how the platform no longer has appropriate incentives. As before, it maximizes its profits by setting its moderation level to $x^*$, where its marginal profit becomes zero. But $x^*$ is always to the right of $x^e$; the platform will leave up some content that is privately profitable but socially harmful.

\begin{pgfecon}{Without liability, a platform undermoderates}{fig:harm1}
  \hplot
%  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  %\plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{4.25}{1.5}{$x^e$}
  \dropline{5.1}{2.6}{$\hat{x}$}
  \draw[dashed, thin] (8, 0) -- (8, -0.5) node[below]{$x^*$};
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:5.1}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:5.1}];
\end{pgfecon}

The standard basic law-and-microeconomic response to harmful conduct is \emph{strict liability}. If a widget factory is forced to compensate everyone who is injured by defective widgets, the factory will take exactly those manufacturing precautions that are cost-justified. Once the factory internalizes the harms it causes, its incentives are aligned with society's.

The same reasoning shows that strict liability gives the platform efficient incentives. Figure \ref{fig:liability1} shows the simple rule of strict liability for a platform that sets its moderation threshold at $\hat{x}$: it is liable for all of the harms caused by the content that it carries (and for none of the harms that would have been caused by content that it could have carried and did not). Under strict liability, the platform's marginal profits are $p(x) - h(x)$, which is identical to the social welfare function. The factory's marginal profit will become zero at $x^e$, exactly when social welfare does, so $x^* = x^e$. 

\begin{pgfecon}{Strict liability}{fig:liability1}
  \hplot
  \dropline{7.5}{4}{$\hat{x}$}
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:7.5}];
\end{pgfecon}


\subsection{Imperfect Information}

The first essential feature that makes intermediary liability more complicated than widget liability a platform has imperfect information about the content that it hosts. Some content is harmful, and other content is not, but they look the same on first glance. A court decides whether a statement is legally defamatory after fact discovery, motion practice, and a trial; a platform does not have the time, the resources, or the power to conduct a full civil lawsuit on every post. A court awards damages in the fullness of time, on relatively complete information. A platform must act now, with radically incomplete information. 

% Thus, platforms don't just \emph{choose} a moderation policy; they also have to \emph{apply} that moderation policy, and in the application they make mistakes. Some harmless content will be taken down, some harmful content will be left up.This fact that mistakes will inevitably follow shapes the initial choice; platforms act under conditions of uncertainty, and act accordingly. And this, in turn, shapes regulators' decisions about liability regimes. Perfect moderation is impossible, so regulators must trade off among differently imperfect moderation regimes.

To capture this point, we now shift from models in which it is the \emph{harmfulness} of content that varies to models in which it is the platform's \emph{knowledge} of how like content is likely to be harmful that varies. Instead of letting the harm from content vary continuously, assume that content comes in two discrete types: harmful and harmless. The harmful content causes a harm to society of $H$ per unit that the platform hosts; the harmless content cases no harms. As in the previous model, all content that the platform hosts generates a profit of $p(x)$ per unit, regardless of whether it is harmful or harmless.

Now, the platform does not know which specific items of content are harmful. Instead, it observes the probability $\lambda(x)$ that a given item of content is harmful. That is, on average, $\lambda(x)$ of the items of content at $x$ are harmful and $1 - \lambda(x)$ of them are harmless. Since the harmful items cause harm $H$ per unit, the expected harm from hosting the content at $x$ is therefore $H\lambda(x)$ per unit.

As before, assume that the same spectrum from  $0$ to $\xmax$ that describes the platform's profits from content also describes the harm from content, except that now what varies is the probability of harm.  That is, $\lambda(x)$ is weakly increasing -- as one moves right along the $x$ axis, the content is increasingly likely to be harmful. It is also useful to assume that the most innocuous content is known with certainty to be harmless, so $\lambda(0) = 0$, and that the most suspicious content is known with certainty to be harmful, so $\lambda(\xmax) = 1$. As before, the model is uninteresting if the most ``harmful'' content is still beneficial overall, so we assume that $p(\xmax) H < 0$. This condition ensures that some content has positive social benefit and some content has negative social benefit, so that there is a real interest in treating them differently.

The point of framing harmfulness this way -- in terms of a probability function $\lambda(x)$ that satisfies these conditions -- is that the resulting expected-harm function $H\lambda(x)$ satisfies exactly the same mathematical properties as $h(x)$ did in the previous model. It starts at $0$ and rises continuously to a value $h(\xmax)$ that is greater than $p(\xmax)$. Thus, the analysis of the platform's moderation decisions -- and especially the proof that strict liability is socially optimal -- is exactly the same.

\begin{pgfecon}{When the platform has imperfect information, it still undermoderates}{fig:harm2}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  %\plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{4.25}{1.5}{$x^e$}
  \dropline{5.1}{2.6}{$\hat{x}$}
  \draw[dashed, thin] (8, 0) -- (8, -0.5) node[below]{$x^*$};
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:5.1}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:5.1}];
\end{pgfecon}

Figure \ref{fig:harm2}, which illustrates this imperfect-information fixed-harm model, is almost identical to Figure \ref{fig:harm1}, which illustrated the perfect-information variable-harm model. The only difference is that the deterministic harm function $h(x)$ has been relabeled as the statistically expected-harm function $H\lambda(x)$.

Now, when the platform chooses a moderation level $\hat{x}$, it is choosing a threshold of \emph{expected} harmfulness. It removes all content whose probability of being harmful is greater than $\lambda(\hat{x})$, and leaves up all content whose probability of being harmful is less than that. Some of the removed content will actually be harmful, and some of it will not. Similarly, some of the content left up will be harmless, and some of it will not. Any choice of $\hat{x}$ (other than the trivial ones to host all content or remove all content) will generate both false positives and false negatives. The choice of a given $\hat{x}$ is a tradeoff between the two.

Figure \ref{fig:harm2} also shows how harmful content reduces social welfare. For a given $\hat{x}$, social welfare consists of two terms. On the one hand, as before, the content that the platform hosts generates profits for the platform: the area under the $p(x)$ curve. But social welfare is now reduced by any content that the platform hosts that actually is harmful. To repeat, at any point $x$, the fraction of content that is harmful is $\lambda(x)$ and the harm per unit of content that is harmful is $H$, so the overall harm is $H\lambda(x)$. Thus the \emph{net} marginal social welfare from hosting content at $x$ is $p(x) - H\lambda(x)$.

The overall harm here is a \emph{statistical} consequence of a given choice of $\hat{x}$. If the platform could perfectly distinguish harmful and harmful content, it could choose to host only the harmless content, and thus society would not suffer the losses associated with the red striped area under the $H\lambda(x)$ curve. (Indeed, we will shortly consider a model under which this distinction is possible, albeit at a cost.) But the point of this model is that the platform cannot distinguish the two. A choice of $\hat{x}$ is a choice about the acceptable ratio of babies to bathwater.

The rest of the analysis is an equally straightforward recapitulation of the previous model. As before, the socially optimal level of moderation $x^e$ is defined by the point at which the social-benefit curve $p(x)$ and the expected-harm curve $H\lambda(x)$ intersect. At this point, the additional harms from the fraction of content that is harmful exactly cancel out the profits and spillovers from all of the content, harmful and harmless. Put algebraically, a regulator should want the platform to set $\hat{x}$ such that $\lambda(\hat{x}) = \frac{s(\hat{x})}{H}$: the greater the harm $H$, the lower the probability $\lambda(x)$ of harm worth tolerating, and thus the lower the appropriate threshold of moderation.

The platform, however, does not have an incentive to set its moderation threshold at the socially optimal level. Instead, as above, its profit-maximizing strategy, is to set the threshold $\hat{x}$ at the point where its marginal profit is exactly zero, i.e. where $p(x) = 0$. Thus $x^* > x^e$ and the platform undermoderates.

And finally, as above, strict liability restores efficiency. Note that for the fraction $1 - \lambda(x)$ of content that is actually harmless, the platform pays no damages. But for the fraction $\lambda(x)$ of content that is harmful, the platform pays the full $H$, for total damages of $H\lambda(x)$. The platform's profit under strict liability becomes $p(x) - H\lambda(x)$, which is equal to social welfare, so it maximizes its profits by setting its moderation threshold to $x^e$, and society is as well-off as it can be, given the indistinguishability of harmful and harmless content.


\subsection{Positive Externalities}

The second essential feature of arguments for platform immunity is that the content platforms carry systematically has positive spillovers for society. A widget factory might come close to capturing the full social value of the widgets it makes. But a platform does not, for at least two reasons.

First, a platform's ``product'' is often not widgets but speech. Speech consists of information, and information is a public good. Once it has been shared with one listener, the speaker cannot easily prevent them from sharing it with others.\footnote{Arrow, Lemley, Frischmann, Baker} A dance video that goes viral on TikTok will be reposted to Twitter and YouTube; the information in a plumbing tutorial will be retained in the minds of viewers and shared with others. All of this third-party value is an externality from the speaker's perspective.

The second source of positive externalities is that platforms do not even capture the full value to speakers of the content they host. As Felix Wu convincingly argues, the value to a user of \emph{posting} content to a platform is typically much larger than the value to the platform of \emph{hosting} that content. A platform does not have an original speaker's incentives. And this point holds true even for non-speech platforms: Airbnb captures only part of the value that apartment hosts extract from rentals made through the platform.

We model these positive externalities by introducing a function $s(x)$, which captures the total value to society of the platform's hosting the $x$-th unit of content. We assume that the ordering of content from best to worst is the same for the platform and for society -- so that $s(x)$ is also weakly decreasing. We also assume that this spillover value is never negative, so that $s(x) \ge p(x)$. (Any negative externalities are already accounted for in by the harm function $H\lambda(x)$.)

\autoref{fig:full1} illustrates the consequences for social welfare when the platform sets its moderation level to $\hat{x}$ in the presence of positive externalities. The green checked area between $p(x)$ and the $x$ axis is the platform's revenue from the content it carries; the blue dotted area between $s(x)$ and $p(x)$ is the additional positive spillovers for society. The red striped area under $H\lambda(x)$ is the harms due to the content the platform carries.

\autoref{fig:full1} also illustrates that neither blanket immunity nor strict liability is necessarily efficient. Under blanket immunity, the platform sets its moderation level to $\xbi$, where $p(x) =0 $. Here, this results in undermoderation, because the platform fails to take into account the harms from the content it carries. But under strict liability, the platform sets its moderation level to $\xsl$, where $p(x) = H\lambda(x)$ and its revenues from carrying additional are exactly cancelled out by the harm that content causes (and hence the damages it must pay). Here, this results in overmoderation, because the platform fails to take into account the spillover benefits from the content it carries.

\begin{pgfecon}{Social welfare in the presence of positive externalities}{fig:full1}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{4.25}{1.625}{$\xsl$}
  \dropline{5.1}{2.6}{$x^e$}
  \dropline{6.5}{4}{$\hat{x}$}
  \draw[dashed, thin] (8, 0) -- (8, -.5) node[below]{$\xbi$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:6.5}];
  \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:6.5}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:6.5}];

 % \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:3.5}];
 % \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:3.5}];
\end{pgfecon}

\subsection{Platform Investigations}

The final moving piece of our model is that a platform can investigate content that it suspects of being harmful. Specifically, the platform can pay a cost $c \ge 0$  per unit of content to investigate and determine with certain whether each item is actually harmful.

To get intuition for how this possibility affects the platform's incentives, start with extreme cases. When investigation is infeasibly costly to ever undertake, i.e. $c \to \inf$, this model collapses into the previous one, because there are no circumstances under which the option to investigate is worth exercising.

On the other hand, when investigation is costless, i.e. $c \to 0$, the platform can  perfectly distinguish harmful content and harmless content. That means it is possible for the platform to take down the harmful content while still leaving up the harmless content. From the regulator's perspective, that is exactly what it should do: take down every piece of harmful content and leave up every every piece of harmless content. Under these circumstances, strict liability for the harms actually caused is again efficient. It is no longer a problem of imperfect information, and the chilling-effect argument for intermediary immunity vanishes.

Now consider what happens for intermediate $c$. The platform has three options for any given item of content: it can leave it up, take it down, or investigate. It is easy to see that the platform will only investigate content where its decision depends on the results of the investigation -- i.e.,  it will take the content down if the investigation reveals it to be harmful, and leave it up otherwise. (If the platform intended to take down the content regardless, it could save $c$ by omitting the investigation, and similarly if it intended to leave up the content regardless.)

Thus the expected value to society for content at $x$ is $0$ if the platform takes down the content, $s(x) - H\lambda(x)$ per unit if it leaves the content up, and $s(x)(1 - \lambda(x)) - c$ per unit if it investigates -- i.e., the value of a harmless piece of content times the probability that the content is harmless. Intuitively, the platform should prefer takedown for content with $\lambda(x)$ close to $1$ and should prefer leaving up for content with $\lambda(x)$ close to $0$, with an interval of investigation somewhere in the middle. 

\begin{pgfecon}{Investigation of intermediate content}{fig:investigate1}
  \lambdaline
  \plotline{harmline}{5}{$H$}
  \plotline{hcline}{4.5}{$H - c$}
  \plotline{cline}{.5}{$c$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{3.55}{1.73}{$\xsl$}
  \dropline{5.2}{2.6}{$x^e$}
  \plotpartialvalue{2}{20}{0}{1.66}{green};
  \plotpartialvalue{2}{20}{1.66}{5.63}{yellow};
  \plotpartialvalue{2}{20}{5.63}{10}{red};
  \plotpartialvalue{3.5}{15}{0}{2.99}{green};
  \plotpartialvalue{3.5}{15}{2.99}{6.87}{yellow};
  \plotpartialvalue{3.5}{15}{6.87}{10}{red};
  
  \draw[domain = .9:10, samples=200, name path = lowerlimit] plot (\x,{5- 5/\x});
  \draw (1, -.5) -- (1,-.5) node[below]{lower limit};
  \draw[domain = 0:9.1, samples=200, name path = upperlimit] plot (\x,{5 /(10 -\x)}) node[right]{upper limit};
  
  \addplot [pattern= dots, pattern color = green] fill between [of = harmline and lambda, soft clip={domain=0:1}];
  \addplot [pattern= dots, pattern color = green] fill between [of = harmline and lowerlimit, soft clip={domain=1:9}];
  \addplot [pattern= dots, pattern color = green] fill between [of = harmline and lambda, soft clip={domain=9:10}];
  
  \addplot [pattern= dots, pattern color = yellow] fill between [of = upperlimit and lowerlimit, soft clip={domain=1.1:9}];
  
  \addplot [pattern= dots, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:1}];
  \addplot [pattern= dots, pattern color = red] fill between [of = upperlimit and axis, soft clip={domain=1:9}];
  \addplot [pattern= dots, pattern color = red] fill between [of = lambda and axis, soft clip={domain=9:10}];
  
\end{pgfecon}


% t/k diagram of actual social welfare from this

The regulator is indifferent between takedown and investigation when (1) the value of the content that investigation will allow to remain up minus the costs of investigation exactly equals (2) the value of taking all content down. (1) consists of the social value $s(x)$ times the fraction of harmless content $1 - lambda(x)$,  minus minus $c$ (times $1$, as the platform must investigate all content, harmful and harmless). (2) is simply $0$. Doing out the math, takedown and investigation are equally efficient when 
 \begin{equation*}\lambda(x) = 1 - \frac{c}{s(x)}.\end{equation*} When $c$ approaches $0$, this converges to $\lambda(x) = 1$, i.e., so the right end of the investigation interval approaches $\xmax$. That is, as the costs of investigation decrease, it is almost always better to investigate than to take down suspected-bad content without first checking. 

The regulator is indifferent between investigation and leaving up when (1) the value of the content that investigation will allow to remain up minus the costs of investigation exactly equals (2) the benefits of all the content minus the costs of the harmful content. (1) is the same as before: $s(x)(1 - \lambda(x)) - c$. (2) simply consists of the social benefits $s(x)$ minus the harms $H\lambda(x)$. Doing out the math, investigating and leaving up are equally efficient when 
\begin{equation*}\lambda(x) = \frac{c}{H - s(x)}.\end{equation*} When $c$ approaches $0$, this converges to $\lambda(x) = 0$, i.e., the left end of the investigation interval approaches $0$. That is, as $c$ decreases, it almost always better to investigate than to leave up the suspected-good content without first checking. 

That is, as $c$ decreases, the ideal investigation interval expands to cover more and more content. On the other hand, for increasing $c$, the investigation interval shrinks and eventually vanishes. \footnote{It vanishes when:
\begin{equation*}
c > \min_{x \in [0, \xmax]} s(x)\frac{H - s(x)}{2s(x) - H}.
\end{equation*}}
When this bound is exceeded, it is never worthwhile from society's perspective for the platform to investigate. It should instead, as in the previous model, act on the basis of the imperfect information it already has.

These results show that a rational regulator should want platforms to invest resources in investigating only when the costs of investigation are sufficiently low, and then only for a range of intermediate cases where the harmfulness of the content is sufficiently unclear. For content that is highly likely or highly unlikely to be harmful, individual investigation is unnecessary and inefficient. Note that this interval contains $x^e$ -- in a sense, affordable investigations expands the cutoff from a sharp on-off to a range warranting a closer look.
% \footnote{In notation, the efficient range of investigation is  \begin{equation*}[\frac{c}{H - s(x)}, 1 - \frac{c}{s(x)}].\end{equation*}}

\autoref{fig:investigate1} illustrates.\footnote{For simplicity of illustration, $\lambda(x)$ is shown as a straight line, but the same results hold in the general case where it is any weakly increasing function that goes from $0$ to $1$ on the interval $[0,\xmax]$.} The curve labeled ``lower limit'' is the dividing line between the region where investigation is better than leaving content up and vice versa. The curve labeled ``upper limit'' is the dividing line between the region where investigation is better than taking content down, and vice versa. These are two-dimensional regions, because whether it is rational to investigate or not depends both on $lambda(x)$ (the horizontal axis) and on $s(x)$ (the vertical axis). As the probability of content being harmful increases (i.e., as one moves horizontally to the right), one starts in a region where it is optimal to leave content up, passes through a region (possibly zero-width) where investigation is optimal, and then moves into a region where it is optimal to take content down. Similarly, as the value of content increases (i.e. as one moves vertically upwards), the optimal policy changes from takedown to investigation to leaving content up. If the curve $s(x)$ passes through the investigation-justified region at all, then $x^e$ lies within it.

\autoref{fig:investigate1} also illustrates the dependence of investigation on $c$. As $c$ decreases, the upper limit moves upwards and the lower limit moves downwards, increasing the size of the (yellow dotted) region where investigation is justified. As $c$ increases, the limits converge, until eventually the region vanishes entirely. In this case, investigation is never justified and we are back to the previous model, where $H\lambda(x)$ marks the dividing line between taking down and leaving up.

% t/k higher c and lower c graphs

A nearly identical analysis applies to a platform's incentives under strict liability.\footnote{Under blanket immunity, a platform will never investigate. Instead, it will always choose to leave all content up.} Because the platform internalizes all the harm that it causes, the only change is to substitute the platform's private profit $p(x)$ for the overall social value $s(x)$. If there is any range for which investigation is justified, it will contain  $\xsl$.) % \footnote{Its interval of investigation is \begin{equation*}[\frac{c}{H - p(x)}, 1 - \frac{c}{p(x)}]\end{equation*}} 
A little algebraic manipulation shows that the platform's preferred interval of investigation is always \emph{shifted left} from the regulator's preferred interval.\footnote{To be precise, at the lower end \begin{equation*}\frac{c}{H - s(x)} < \frac{c}{H - p(x)},\end{equation*} and at the upper end \begin{equation*}1 - \frac{c}{p(x)} < 1 - \frac{c}{s(x)}.\end{equation*}} Intuitively, because the platform has less at stake, it will be more likely to remove content rather than investigating and also more likely to investigate content rather than leaving it up.



\section{Platform Liability and Platform Immunity}

We have already seen that neither blanket immunity nor strict liability is guaranteed to be efficient. This Part provides a comparative analysis of the arguments for and against other liability regimes. The point is not to definitely settle on one or another as optimal, but instead to bring out the intuitions behind each and to get a sense of the conditions they depend on.



\subsection{Blanket Immunity}

The fundamental challenge of platform liability law is that content has both harms and benefits to society that the platform does not internalize. A profit-maximizing platform makes its decisions based on how much it can make from hosting content, paying no attention to either positive or negative spillovers. In the absence of liability -- a legal regime we refer to as \emph{blanket immunity} -- either of these effects can dominate, so both overmoderation and undermoderation are possible.

Figure \ref{fig:harm2} shows that blanket immunity can lead a platform to \emph{undermoderate}: the platform makes money from hosting content that is, on average, a net negative for society.\footnote{For much of this section, for the sake of simplicity of exposition, we will discuss platform investigations only when directly relevant.} In this region between $x^e$ and $x^*$ (red and striped), there are both harmful and harmless items of content, but the harm from the harmful items outweighs the benefits of all the content together. Making the platform liable for the harmful content it carries could potentially improve matters, by encouraging it to carry less content.

\begin{pgfecon}{Blanket immunity can lead to undermoderation}{fig:blanket1}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{5.1}{2.6}{$x^e$};
  \draw[dashed, thin] (8, 4.75) -- (8, -0.5) node[below]{$x^*$};
  %\draw[dashed, thin] (9.25, 0) -- (9.25, 0.5) node[above]{$x^e$};
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=0:5.1}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and welfare, soft clip={domain=5.1:8}];

 % \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:3.5}];
 % \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:3.5}];
\end{pgfecon}

But \ref{fig:blanket2} shows that even under blanket immunity, a platform might also \emph{overmoderate}. If the platform's revenues too small compared with the social value of the content it carries, it may simply not be economically feasible for the platform to carry all the content it would be socially valuable to.

\begin{pgfecon}{Blanket immunity can lead to undermoderation}{fig:blanket2}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{welfare}{4.5}{12}{$s(x)$}
  \plotvalue{profit}{.25}{30}{$p(x)$}
  \draw[dashed, thin] (4.25, 3.85) -- (4.25, 0) node[below]{$x^*$};
  \draw[dashed, thin] (5.55, 3.15) node[above]{$x^e$} -- (5.55, 0) ;
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=0:4.25}];
  \addplot [pattern= crosshatch, pattern color = yellow] fill between [of = welfare and lambda, soft clip={domain=4.25:5.55}];
%  \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:8}];
%  \addplot [pattern= north east lines, pattern color = red] fill between [of = welfare and axis, soft clip={domain=8:9.25}];
\end{pgfecon}

It is technically possible for these effects to cancel out, so that the platform arrives at an appropriate level of moderation on its own. But there is no particular reason to expect that this will be the case. Instead, a particular platform, hosting particular types of content, with particular harms and benefits, will typically fall on one side or the other.

The policy responses to overmoderation and undermoderation are different. Overmoderation is typically addressed with subsidies or must-carry rules to force platforms to carry content they rather would not on their own. Undermoderation is typically addressed by imposing liability on platforms. We take up these two possibilities in turn, starting with overmoderation.


\subsection{Overmoderation: Subsidies}

Many responses to overmoderation are familiar from telecommunications and intellectual-property law. One of the most common is \emph{subsidies}, in which the government pays the platform to carry content. \autoref{fig:subsidies1} shows a case in which the government gives the platform a subsidy of $\epsilon$ for any content that it carries. This pushes the platform's profits up to the point that $x^* = x^e$ and it carries the socially optimal level of content. 

There are at least three challenges in providing subsidies. First, the regulator must accurately estimate $x^e$, which requires an understanding both of the value of content $s(x)$ and its harms $H\lambda(x)$. Second, the regulator must choose an appropriate subsidy $\epsilon$, which requires an understanding of the platform's revenues $p(x)$. And third, the subsidy must be one that the regulator is willing to pay. The orange dotted region in \autoref{fig:subsidies1} is money that must come from somewhere. It is not a welfare loss to society, just a wealth transfer (ignoring administrative costs and the distortionary effects of taxation, that is). Below-cost mail service is an example of this type of subsidy.


\begin{pgfecon}{Flat subsidies}{fig:subsidies1}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{welfare}{4.5}{12}{$s(x)$}
  \plotvalue{profitplus}{.55}{30}{$p(x) + \epsilon$}
  \plotvalue{profit}{.25}{30}{}
  \draw (10, -2) node {$p(x)$};
  \draw[dashed, thin] (5.55, 3.15) node[above]{$x^e$} -- (5.55, -.75) node[below]{$x^*$};
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=0:5.55}];
  \addplot [pattern= crosshatch dots, pattern color = orange] fill between [of = profitplus and profit, soft clip={domain=0:5.55}];
\end{pgfecon}

A partial solution to the third problem is \emph{targeted subsidies}. Here, the government subsidizes content only in the range where subsidies make a difference in the platform's decision of whether to carry it -- between $x^*$ and $x^e$. This reduces the size of the subsidies required, but it increases the difficulty of the regulatory problem, because now the regulator must be able to accurately estimate  $x^*$, and not just know the behavior of $p(x)$ in the neighborhood of $x^e$. The FCC's Universal Service Fund is a targeted subsidy. It helps make broadband Internet access more widely available by supporting its availability to people and communities who for whom it would not otherwise be profitable for telecom companies to provide it.

\begin{pgfecon}{Targeted subsidies}{fig:subsidies2}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{welfare}{4.5}{12}{$s(x)$}
  \plotvalue{profitplus}{.55}{30}{$p(x) + \epsilon$}
  \plotvalue{profit}{.25}{30}{}
  \draw (10, -2) node {$p(x)$};
  \draw[dashed, thin] (4.25, 3.85) -- (4.25, 0) node[below]{$x^*$};
  \draw[dashed, thin] (5.55, 3.15) node[above]{$x^e$} -- (5.55, -.75);
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=0:5.55}];
  \addplot [pattern= crosshatch dots, pattern color = orange] fill between [of = profitplus and profit, soft clip={domain=4.25:5.55}];
\end{pgfecon}

Subsidies can also be provided indirectly, by subsidizing the users who create content and distribute it through platforms, and the consumers who receive it. The idea here is that if distribution is more valuable to creators and consumers, they will be willing to pay more to distributors, thus shifting the $p(x)$ curve upwards. There is an argument that the copyright system has some of these features, although it is not typically described in these terms.

\subsection{Overmoderation: Must-Carry}

Another response to overmoderation is to impose a \emph{must-carry} rule, in which the platform must host all content submitted to it. Formally, the regulator forces the platform to set $\hat{x} = \xmax$, i.e. the far right of the diagram. Compared to subsidies, a must-carry system is simpler to design and almost by definition requires less outlay. It also removes discretion from the platform, which may be a concern if the platform has a conflict of interest due to other business lines or does not agree with the regulator's understanding of which content is valuable. Something like this, for example, is a commonly advanced argument for network neutrality.

A must-carry rule, however, must satisfy two conditions to be justified compared with the baseline. First, it must actually result in hosting more worthwhile than worthless content. In \autoref{fig:mustcarry1}, the upper green gridded region is the positive-value content that must-carry causes to be hosted, and the upper red striped region is the negative-value content it also causes to be hosted. If the red region is larger than the green one, must-carry is counter-productive; the bad additional content outweighs the good.\footnote{This analysis omits the investigation option, because it is never rational for a platform to investigate content it is just going to leave up anyway.}

\begin{pgfecon}{Must-carry}{fig:mustcarry1}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{welfare}{4.5}{20}{$s(x)$}
  \plotvalue{profit}{.25}{30}{$p(x)$}
  \draw[dashed, thin] (4.25, 4) -- (4.25, 0) node[below]{$x^*$};
  \draw[dashed, thin] (5.9, 3.5) node[above]{$x^e$} -- (5.9, 0) ;
  
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=4.25:5.9}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and welfare, soft clip={domain=5.9:10}];

  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:4.25}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = axis and profit, soft clip={domain=4.25:10}];
  
\end{pgfecon}

A little more subtly, must-carry can also counter-productively drive a platform out of the market. In \autoref{fig:mustcarry1}, the lower green gridded region is the profit's profits from hosting the content it wants to, and the lower red striped region it is losses from hosting the content it is forced to. If the red region is larger than the green one, it is unprofitable for the platform to operate at all, and it will rationally shut down rather than comply with a must-carry mandate.

\subsection{Overmoderation: Lawful Must-Carry}

One common criticism of must-carry -- especially for application-layer platforms -- is that it compels platforms to carry content that society itself considers harmful, even illegal. So it is common to see must-carry mandates limited to ``lawful'' content. The FCC's Obama-era network neutrality regulations had such a carveout, as do the Texas and Florida social-media must-carry bills whose constitutionality is currently being litigated.\footnote{t/k}

We can model a \emph{lawful must-carry} rule by stating that the platform \emph{must} host all harmless content, but has discretion whether or not to host harmful content. Of course, to know with certainty whether content is harmless, the platform must investigate it. Thus, under lawful must-carry, the platform has two choices: it can either leave the content up without investigation, or it can investigate it and take it down if harmful. 

As \autoref{fig:mustcarry2} illustrates, the platform's marginal revenue from leaving up is $p(x)$, and its marginal revenue from investigation is $(1 - \lambda(x))p(x) -c $. Thus, the platform finds the two equivalent when $p(x) = \frac{-c}{\lambda(x)}$, which can only occur when the platform's profit $p(x)$ has gone negative. The intersection of these two curves (if they meet at all), which we call $\xsubi$, is to the right of $x^*$. To the left of $\xsubi$, the platform leaves up content, so its profits and social welfare are as above. But to the right of $\xsubi$, the platform investigates all content and takes down all harmful content. Compared with a flat must-carry requirement, the platform can reduce its losses from the content it is compelled to carry, and thus may be better able to keep operating in the face of a lawful must-carry requirement.

Lawful must-carry can also be better for social welfare, because the platform will filter out some content that is both harmful and unprofitable.  \autoref{fig:mustcarry2} shows that the welfare effects can be subtle and complex. $\xsubi$ creates a discontinuity. To the left, social welfare is the benefits of all content $s(x)$ minus the harms of all content $H\lambda(x)$. To the right, 
investigation eliminates the harms $H\lambda(x)$ but introduces two new costs: the cost of foregone benefits from removed harmful content $s(x)\lambda(x)$ and the costs of investigation $c$. 

\begin{pgfecon}{Lawful must-carry}{fig:mustcarry2}
  \lambdaline
  \plotline{harmline}{5}{$H$}
  %\plotline{hcline}{4.5}{$H - c$}
  %\plotline{cline}{-.5}{$-c$}
  \plotvalue{profit}{.5}{30}{$p(x)$}
  \plotvalue{welfare}{4.5}{20}{$s(x)$}
  \draw[dashed, thin] (5.4, 3.6)  -- (5.4, 0) node[below]{$x^*$};
  \draw[dashed, thin] (6.5, 3.2) node[above]{$x^e$} -- (6.5, 0) ;
  \draw[dashed, thin] (7.5, 3.75)  -- (7.5, -.75)node[below]{$\xsubi$} ;
    
  %\draw[domain = 2.5:10, samples=200, name path = lowerlimit] plot (\x,{-5/\x}) node[right]{$\frac{-c}{\lambda(x)$};
  
  \draw[domain = 0:10, samples=200, name path = filterprofit] plot 
  (\x,{-.5 + (1 - (\x/10)) * (\x < 1.5 ? .5 : 
   (\x < 6.5 ? .5 - (\x - 1.5)^2 / 30 : 
   .5 - (25 / 30) - (10 * (\x - 6.5)) / 30 ))}) node[right]{$(1 - \lambda(x))p(x) -c$};

  \draw[domain = 0:10, samples=200, name path = filterwelfare] plot 
  (\x,{.5 + (\x / 10) * (\x < 1.5 ? 4.5 : 
   (\x < 6.5 ? 4.5 - (\x - 1.5)^2 / 20 : 
   4.5 - (25 / 20) - (10 * (\x - 6.5)) / 20 ))}) node[right]{$s(x)\lambda(x) + c$};
  
   \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:5.4}];
   \addplot [pattern= north east lines, pattern color = red] fill between [of = axis and profit, soft clip={domain=5.4:7.5}];
   \addplot [pattern= north east lines, pattern color = red] fill between [of = axis and filterprofit, soft clip={domain=7.5:10}];
      
   \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=5.4:6.5}];   
   \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and welfare, soft clip={domain=6.5:7.5}];
   \addplot [pattern= grid, pattern color = green] fill between [of = welfare and filterwelfare, soft clip={domain=7.5:8}];
   \addplot [pattern= north east lines, pattern color = red] fill between [of = filterwelfare and welfare, soft clip={domain=8:10}];
    
\end{pgfecon}

\subsection{Undermoderation: Strict Liability}

We have seen that strict liability for platforms is not necessarily efficient. But it is possible to be more precise about how and why.

First, while immunity can lead to overmoderation or undermoderation, strict liability always leads to overmoderation.  The reason is simple. Strict liability causes the platform to internalize the harms from the content it carries, but not the offsetting benefits. This asymmetry between harms (for which it faces liability) and benefits (for which it is not compensated) pushes the platform to to remove more content than an omniscient regulator would.

Second, this overmoderation fundamentally depends on the platform's imperfect information about content. If the platform could costlessly distinguish harmless and harmful content, then strict liability would be efficient, as it would be feasible to expect the platform to separate the two and remove only the harmful content. But given imperfect information, the platform \emph{cannot tell with certainty} which content is harmless and creates net positive externalities and which content is harmful and creates net negative externalities. A platform facing strict liability consistently overmoderates. This overmoderation expresses itself in the removal of harmless content.

Thus, our model validates Felix Wu's argument for intermediary immunity. The combination of (1) positive externalities and (2) imperfect information causes a platform subject to strict liability to engage in collateral censorship. The platform has less at stake than an original speaker (positive externalities) and responds by removing good content as well as bad (imperfect information). These conditions are jointly necessary and sufficient; if there are no positive externalities (i.e. $(s(x) = p(x)$ or the platform has perfect information (i.e. $c = 0$), then strict liability is efficient.

It is important to note that the positive externalities here are not just the positive externalities common to all speech. As Wu explains, speech law already provides heightened protections for original speakers -- and yet intermediaries have protections that are higher still.\footnote{Wu at 304.} Speakers have private motivations for speaking: financial, self-expression, reputation-building, community-building, or even revenge. Platforms share their speech but not their motivations.

Platforms also differ from speakers in that speakers generally have much better information about the harmfulness of their speech. A speaker knows whether there is a factual basis for allegations of corruption or harassment; a platform does not. A speaker knows whether they wrote a song themselves or copied it from someone else; a platform does not. In terms of our model, a speaker has $c = 0$ or close to it for their own speech, and thus is much less likely to be chilled from harmless speech by the threat of liability for harmful speech.

Whether social welfare is higher under strict liability or immunity depends on the parameters of the model:  $p(x)$, $s(x)$, $H$, $\lambda(x)$, and $c$. Strict liability always leads to overmoderation; immunity could either undershoot or overshoot the efficient level of moderation. Generally speaking, a blanket immunity regime is most justified when there there are large positive externalities (a large difference between $p(x)$ and $s(x)$), highly imperfect information ($c$ is high and $\lambda(x)$ has a large intermediate region that is not close to $0$ or to $1$), and socially harmful content is also unprofitable ($p(x) = 0$ at a point to the left of the point at which $s(x) = H\lambda(x)$). There is a strong argument that these conditions describe many categories of content moderation today.


\subsection{Undermoderation: Liability on Notice}

At common law, a ``distributor'' of defamatory speech published by a third party (e.g. a bookstore) was liable ``if, but only if, [it] knows or has reason to know of its defamatory character.''\footnote{Restatment (Second) of Torts ยง~581(1).}  Section 512(c)(1)(A)(i) removes a platform's immunity as to specific material if it has ``actual knowledge that the material \ldots is infringing''\footnote{17 U.S.C. ยง 512(c)(1)(A)(i).} and the platform does not ``act[] expeditiously to remove, or disable access to, the material.''\footnote{17 U.S.C. ยง 512(c)(1)(A)(iii).} 

These are examples of \emph{liability on notice}: a platform is liable for harmful content that it hosts, but only when it has specific knowledge that a particular item is harmful. The intuition behind a liability-on-notice regime is that while it might not be feasible to require a platform to \emph{acquire} the knowledge to show that an item of content is harmful on its own, once the platform \emph{has} such knowledge (from whatever source derived) it is reasonable to expect it to take action on it. If someone else is willing to bear the expense of investigating content, then from the platform's perspective, it receives investigation for free.

The most straightforward way to model liability on notice in our framework is to introduce additional agents: the \emph{victims} of harm, who can investigate content and provide notice to the platform. In this modification, each individual item of content is indexed to a distinct victim; that victim suffers the harm $H$ per unit if that item is harmful and the platform carries it. The victims, like the platform, can investigate content at a cost of $c_v$ per unit. The victims are also able to send notices to the platform for any content they choose, and the platform is liable to the relevant victim for any harm that victim suffers from content about which the platform has received a notice.\footnote{All parties can observe the functions $p(x)$, $s(x)$, and $\lambda(x)$, and the parameters $H$, $c$, and $c_v$.}

Intuitively, it seems liability on notice should induce the state of affairs depicted in \autoref{fig:notice1}. For content at $x$, the relevant victim  has the option of doing nothing and suffering harm $H\lambda(x)$ or of investigating at cost $c_v$ and giving the platform notice if the content is harmful. For low $\lambda(x)$ they prefes to suffer the harm; for high $\lambda(x)$ they prefer to investigate, with crossover at the point $x^v$ for which $\lambda{x^v} = \frac{c_v}{H}$. The platform will always remove any harmful content for which it receives a notice, because a costless removal is better than paying to compensate a harm $H$ that outweighs its profits $p(x)$. Thus the platform never actually has to pay compensation. (The platform cuts off hosting content at all at the point where its profits go negative.) The red striped region shows victims' uncompensated harms/losses. The blue dotted region above it shows the social surplus.

\begin{pgfecon}{Naive model of liability on notice}{fig:notice1}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotline{cline}{.5}{$c_v$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{2.8}{.5}{$x^v$}
  \dropline{8}{.8}{$x^*$}
   \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and lambda, soft clip={domain=0:2.8}];
   \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and cline, soft clip={domain=2.8:8}];
   \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:2.8}];
   \addplot [pattern= north east lines, pattern color = red] fill between [of = cline and axis, soft clip={domain=2.8:8}];
\end{pgfecon}

But this reasoning is wrong. The problem is that victims \emph{are not restricted to sending notices for content that actually is harmful}. They victim have a third option for content besides ignoring it and investigating it -- they can also send a notice without investigation. In economic terms, liability on notice creates a signaling game between victims and platform. For each item of content, the relevant victim chooses whether to ignore it, investigate and give notice if the content is harmful, or give notice without investigation. The platform either does or does not receive a notice, and then chooses whether to take the content down, investigate it, or leave it up.

Under these assumptions, notice on takedown collapses into strict liability. The victim never investigates but always sends a notice. Because the platform receives a notice regardless of whether the content is harmful or not, the notices are of no use to the platform in distinguishing harmful from harmless content. At the same time, the platform is now legally on notice of all harmful content, and thus subject to strict liability for failure to remove it. The platform faces exactly the same incentives, with exactly the same knowledge, and exactly the same options as in the strict liability case. The notices do no useful work.

This analysis bears out academic criticism of the Section 512(c) notice-and-takedown regime. Copyright claimants frequently send notices based on no or minimal investigation, including on content that involves no reuse of copyrightable expression or is obviously a fair use.

One way to make the signal provided by a notice more credible is to make it more expensive to send notices against harmless material. Section 512(f) tries to do this by imposing liability on anyone who ``knowingly materially misrepresents'' that material is infringing in a takedown notice. Unfortunately, judicial interpretations have almost completely defanged this remedy. Courts have held that a subjective belief of infringement -- however unreasonable -- is a sufficient defense to a 512(f) suit. They have also held that even the most cursory investigative process is sufficient. These courts have reasoned that they would not want to copyright owners considering sending takedowns to be detered by the fear of liability. This is a chilling effect caused by the indistinguishability of harmful and harmless content, so the concern is real. But at the same time, notices must function as signals to be useful.

Another way to make a notice more useful is to require it to contain specific evidence of harmfulness, thereby making the platform's own investigation cheaper. A claim of copyright infringement requires proof that (1) particular material (2) uses a copyrighted work (3) in a way that infringes. In the abstract, investigation is expensive because a platform must investigate all of its content, compare that content to all copyrighted works, and consider all possible justifications (such as licenses, fair use, etc.).  The statutory template for a takedown notice addresses these elements by requiring, respectively, ``[i]dentification of the material that is claimed to be infringing \ldots and information reasonably sufficient to permit the service provider to locate the material.''\footnote{17 U.S.C. ยง 512(c)(3)(A)(iii).}, ``[i]dentification of the copyrighted work claimed to have been infringed,'',\footnote{17 U.S.C. ยง 512(c)(3)(A)(ii).}, and ``[a] statement that the complaining party has a good faith belief that use of the material \ldots  is not authorized by the copyright owner, its agent, or the law.''\footnote{17 U.S.C. ยง 512(c)(3)(A)(v).}

But experience has shown that these three requirements stand on somewhat different footing. Courts have generally been unwilling to relax the requirement of identification of specific material, recognizing that without that specific identification the platform must investigate a vast array of content.\footnote{Perfect 10, etc.} And plaintiffs have also been held to the requirement that they identify the relevant copyrighted works. (Indeed, in a world where copyright subsists on fixation, almost every upload will contain material that is copyrighted by someone, so that all of the important questions about the copyright itself go to whether the uploader had the right to do so.) But, as noted above, courts have held that the ``good faith belief'' required by Section 512(c)(3)(A)(v) can be satisfied by a subjective belief, regardless of whether that belief is reasonable or not -- and even if the notice-sender acts in bad faith, the damages against them are likely to be nominal at best.\footnote{Rossi, Lenz} 

This analysis also shows why commentators have generally regarded liability on notice as producing similar chilling effects to strict liability, even outside the copyright space.\footnote{Wu, etc.} It is simply too easy to send a notice against content that is not actually harmful. Proposals to instate some kind of liability on notice need to affirmatively demonstrate that the notices they allow will be credible signals.


\subsection{The Passage of Section 230}


Now we are in a position to appreciate the crucial policy arguments at the heart of Section 230. Famously, Section 230 was enacted against the backdrop of two judicial decisions on the liability of online intermediaries, \emph{Cubby v. Compuserve} and \emph{Stratton Oakmont v. Prodigy}. In \emph{Cubby}, the court held that CompuServe could not be held liable for user-posted content where it "neither knew nor had reason to know" that the content was defamatory.\footnote{Cubby at 141.} But in \emph{Stratton Oakmont}, the court held that Prodigy could be held liable for user-posted content, even where it lacked such knowledge. Both courts treated the cases involving imperfect information -- the issue was how a platform \emph{without} specific knowledge should be treated.

Notoriously, the \emph{Stratton Oakmont} court distinguished \emph{Cubby} on the grounds that Prodigy's "conscious choice, to gain the benefits of editorial control, has opened it up to a greater liability than CompuServe and other computer networks that make no such choice."\footnote{Stratton Oakmont at t/k} On this reasoning, services like CompuServe that exercise no "editorial control" do not face strict liability, whereas moderated services like Prodigy do.

In terms of our model, the rule in \emph{Stratton Oakmont} puts platforms to a choice. If they host \emph{all} content (i.e., set $x^* = n$), they face no liability. But if they moderate \emph{any} content (i.e., set $x^* < n$), they are strictly liable for the harms caused by any content they do not remove.  But such a legal regime creates a powerful incentive not to moderate at all -- indeed, of the two choices, the \emph{Cubby} immunity no moderation at all will always be more profitable than moderation with \emph{Stratton Oakmont} liability. The \emph{Stratton Oakmont} court believed that it was giving Prodigy an incentive to move $x^*$ to the left, closer to $x^e$. But in the presence of the \emph{Cubby} alternative, the Prodigies of the world will be more tempted to move to the opposite extreme, giving up on moderation altogether.

This is why Section 230(c) is titled ``Protection for `Good Samaritan' blocking and screening of offensive material.'' It was enacted to remove the perverse disincentive to moderation created by the rule of \emph{Cubby}. A platform protected by Section 230 is now free to move $x^*$ off the right endpoint without fear that it will now open itself to liability and be forced to move much further to the left. It might do so out of civic-mindedness, to reduce public-relations trouble, or (as Prodigy did) to create a better experience for users.\footnote{A downward-sloping $P$ curve that crosses the $x$ axis would capture these motivations.}

%\subsection{Negligence}


% 
% negligence
% - NYT v. Sullivan?
%
% conditional immunity
% 





% 
% 
% (pre-230) must-carry OR strict liability
% 
% Section 230
%   and reforms thereof
% 
% Section 512
% 
% DMA










\newcommand{\xmax}{x_{\text{max}}}
\newcommand{\xbi}{x_{\text{BI}}^*}
\newcommand{\xsl}{x_{\text{SL}}^*}

\section{An Economic Model of Moderation}

There are two distinctive features of platform liability for harmful third-party content. The platform has \emph{imperfect information} about which content is harmful and which is not, and  content can have \emph{positive externalities} not captured by the platform itself, and  These two features, taken together, mean that holding the platform liable for the harmful content it carries can go wrong. Because the platform cannot perfectly distinguish harmful from harmless content, and because it does not internalize the full benefits from the harmless content, the thread of liability can cause the platform to overmoderate, removing too much harmless content along with the harmful content. To see how these effects arise, it is useful to start with a model in which they are absent.


\subsection{Harmful Content}

Imagine the content submitted by users to a platform arranged on a spectrum from worthwhile to worthless. At one end, the content is entertaining and informative -- cat pictures and civics lessons. At the other end, the content is stomach-churning or worse -- gross-out pictures and badly-written spam. A platform sets its moderation policy by deciding where along this spectrum to draw the line.

More formally, let the spectrum of content run from $0$ to $\xmax$ and let the marginal profit that the platform can make from hosting the $x$-th unit of content be $p(x)$. To capture the idea that the content is ordered from best to worst, assume that $p(x)$ is weakly decreasing -- i.e. is flat or slopes down as $x$ increases. Content for which $p(x) <0$ generates negative marginal profit -- that is, this content costs the platform more to host than it brings in. The platform chooses a moderation level $\hat{x}$. It hosts all content for which $x \le \hat{x}$ and takes down all content for which $x > \hat{x}$. As Figure \ref{fig:host1} illustrates, the platform's total profit will be the area under the curve $p(x)$ between $0$ and $\hat{x}$. 

\begin{pgfecon}{A platform chooses its moderation level}{fig:host1}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \dropline{4.28}{1.65}{$\hat{x}$};
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  % \addplot [pattern= north east lines, pattern color = red] fill between [of = welfare and lambda, soft clip={domain=4.28:5.4}];
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:4.28}];
\end{pgfecon}

In this situation, a rational platform will choose the value of $\hat{x}$ that maximizes its profits. As Figure \ref{fig:host2} illustrates, this occurs at the point $x^*$ where $p(x)$ crosses the $x$ axis and becomes negative. If the platform chooses $hat{x}$ to the left of $x^*$, each unit of content it adds will generate positive marginal profit, so it could make more money by hosting more content. But if the platform chooses $hat{x}$ to the right of $x^*$, the marginal unit of content generates negative marginal profit, so the platform can save money by hosting less content. Thus, $x^*$ is the profit-maximizing level of moderation. 

By the same reasoning, the point where $p(x)$ crosses the $x$ axis is also the socially efficient level of moderation. Because there are no externalities from hosting content, overall social welfare simply is the platform's profits. Thus, $x^e$, the socially efficient level of moderation, is equal to $x^*$, and the platform's incentives are aligned with society's.

\begin{pgfecon}{Without externalities, the platform has efficient incentives}{fig:host2}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \draw[dashed, thin] (8, 0) -- (8, -0.5) node[below]{$x^*=x^e$};
\end{pgfecon}

Regulation potentially becomes appropriate when the platform's incentives diverge from society's. The essential premise on which any form of liability depends is that some conduct is harmful. For a platform, that conduct is content. 

To model harmful content, we add a function $h(x)$ that measures how much harm the $x$-th unit of content causes to third parties. We assume that the same spectrum from that captures the platform's profits also describes the content's harmfulness.As one moves right along the $x$ axis, $p(x)$ falls and $h(x)$ rises. The platform's marginal profits are still $p(x)$; marginal social welfare is now $p(x) - h(x)$. To make the model interesting, we assume that the most innocuous content is completely harmless (i.e. $h(0) = 0$) and that the most problematic content is unambiguously harmful to society (i.e.  $p(\xmax) < h(\xmax)$). These conditions ensure that some content has positive social benefit and some content has negative social benefit, so that there is a real interest in treating them differently. It also follows, by the intermediate value theorem, that there is some point at which the curves $p(x)$ and $h(x)$ cross.

In Figure \ref{fig:harm1}, if the platform sets its moderation level at $\hat{x}$, then the green checked region beneath $p(x)$ corresponds to its profits and the red striped region beneath $h(x)$ corresponds to the harms caused by the content it carries. The efficient moderation level is at $x^e$, the point where they cross, where the marginal social welfare of one additional unit of content is zero. To the left of $x^e$, content is good because $p(x) > h(x)$; to the right of $x^e$, content is bad because $h(x) > p(x)$.

Figure \ref{fig:harm1} also illustrates how the platform no longer has appropriate incentives. As before, it maximizes its profits by setting its moderation level to $x^*$, where its marginal profit becomes zero. But $x^*$ is always to the right of $x^e$; the platform will leave up some content that is privately profitable but socially harmful.

\begin{pgfecon}{Without liability, a platform undermoderates}{fig:harm1}
  \hplot
%  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  %\plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{4.25}{1.5}{$x^e$};
  \dropline{5.1}{2.6}{$\hat{x}$};
  \draw[dashed, thin] (8, 0) -- (8, -0.5) node[below]{$x^*$};
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:5.1}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:5.1}];
\end{pgfecon}

The standard basic law-and-microeconomic response to harmful conduct is \emph{strict liability}. If a widget factory is forced to compensate everyone who is injured by defective widgets, the factory will take exactly those manufacturing precautions that are cost-justified. Once the factory internalizes the harms it causes, its incentives are aligned with society's.

The same reasoning shows that strict liability gives the platform efficient incentives. Figure \ref{fig:liability1} shows the simple rule of strict liability for a platform that sets its moderation threshold at $\hat{x}$: it is liable for all of the harms caused by the content that it carries (and for none of the harms that would have been caused by content that it could have carried and did not). Under strict liability, the platform's marginal profits are $p(x) - h(x)$, which is identical to the social welfare function. The factory's marginal profit will become zero at $x^e$, exactly when social welfare does, so $x^* = x^e$. 

\begin{pgfecon}{Strict liability}{fig:liability1}
  \hplot
  \dropline{7.5}{4}{$\hat{x}$};
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:7.5}];
\end{pgfecon}


\subsection{Imperfect Information}

The first essential feature that makes intermediary liability more complicated than widget liability a platform has imperfect information about the content that it hosts. Some content is harmful, and other content is not, but they look the same on first glance. A court decides whether a statement is legally defamatory after fact discovery, motion practice, and a trial; a platform does not have the time, the resources, or the power to conduct a full civil lawsuit on every post. A court awards damages in the fullness of time, on relatively complete information. A platform must act now, with radically incomplete information. 

% Thus, platforms don't just \emph{choose} a moderation policy; they also have to \emph{apply} that moderation policy, and in the application they make mistakes. Some harmless content will be taken down, some harmful content will be left up.This fact that mistakes will inevitably follow shapes the initial choice; platforms act under conditions of uncertainty, and act accordingly. And this, in turn, shapes regulators' decisions about liability regimes. Perfect moderation is impossible, so regulators must trade off among differently imperfect moderation regimes.

To capture this point, we now shift from models in which it is the \emph{harmfulness} of content that varies to models in which it is the platform's \emph{knowledge} of how like content is likely to be harmful that varies. Instead of letting the harm from content vary continuously, assume that content comes in two discrete types: harmful and harmless. The harmful content causes a harm to society of $H$ per unit that the platform hosts; the harmless content cases no harms. As in the previous model, all content that the platform hosts generates a profit of $p(x)$ per unit, regardless of whether it is harmful or harmless.

Now, the platform does not know which specific items of content are harmful. Instead, it observes the probability $\lambda(x)$ that a given item of content is harmful. That is, on average, $\lambda(x)$ of the items of content at $x$ are harmful and $1 - \lambda(x)$ of them are harmless. Since the harmful items cause harm $H$ per unit, the expected harm from hosting the content at $x$ is therefore $H\lambda(x)$ per unit.

As before, assume that the same spectrum from  $0$ to $\xmax$ that describes the platform's profits from content also describes the harm from content, except that now what varies is the probability of harm.  That is, $\lambda(x)$ is weakly increasing -- as one moves right along the $x$ axis, the content is increasingly likely to be harmful. It is also useful to assume that the most innocuous content is known with certainty to be harmless, so $\lambda(0) = 0$, and that the most suspicious content is known with certainty to be harmful, so $\lambda(\xmax) = 1$. As before, the model is uninteresting if the most ``harmful'' content is still beneficial overall, so we assume that $p(\xmax) H < 0$. This condition ensures that some content has positive social benefit and some content has negative social benefit, so that there is a real interest in treating them differently.

The point of framing harmfulness this way -- in terms of a probability function $\lambda(x)$ that satisfies these conditions -- is that the resulting expected-harm function $H\lambda(x)$ satisfies exactly the same mathematical properties as $h(x)$ did in the previous model. It starts at $0$ and rises continuously to a value $h(\xmax)$ that is greater than $p(\xmax)$. Thus, the analysis of the platform's moderation decisions -- and especially the proof that strict liability is socially optimal -- is exactly the same.

\begin{pgfecon}{When the platform has imperfect information, it still undermoderates}{fig:harm2}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  %\plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{4.25}{1.5}{$x^e$};
  \dropline{5.1}{2.6}{$\hat{x}$};
  \draw[dashed, thin] (8, 0) -- (8, -0.5) node[below]{$x^*$};
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:5.1}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:5.1}];
\end{pgfecon}

Figure \ref{fig:harm2}, which illustrates this imperfect-information fixed-harm model, is almost identical to Figure \ref{fig:harm1}, which illustrated the perfect-information variable-harm model. The only difference is that the deterministic harm function $h(x)$ has been relabeled as the statistically expected-harm function $H\lambda(x)$.

Now, when the platform chooses a moderation level $\hat{x}$, it is choosing a threshold of \emph{expected} harmfulness. It removes all content whose probability of being harmful is greater than $\lambda(\hat{x})$, and leaves up all content whose probability of being harmful is less than that. Some of the removed content will actually be harmful, and some of it will not. Similarly, some of the content left up will be harmless, and some of it will not. Any choice of $\hat{x}$ (other than the trivial ones to host all content or remove all content) will generate both false positives and false negatives. The choice of a given $\hat{x}$ is a tradeoff between the two.

Figure \ref{fig:harm2} also shows how harmful content reduces social welfare. For a given $\hat{x}$, social welfare consists of two terms. On the one hand, as before, the content that the platform hosts generates profits for the platform: the area under the $p(x)$ curve. But social welfare is now reduced by any content that the platform hosts that actually is harmful. To repeat, at any point $x$, the fraction of content that is harmful is $\lambda(x)$ and the harm per unit of content that is harmful is $H$, so the overall harm is $H\lambda(x)$. Thus the \emph{net} marginal social welfare from hosting content at $x$ is $p(x) - H\lambda(x)$.

The overall harm here is a \emph{statistical} consequence of a given choice of $\hat{x}$. If the platform could perfectly distinguish harmful and harmful content, it could choose to host only the harmless content, and thus society would not suffer the losses associated with the red striped area under the $H\lambda(x)$ curve. (Indeed, we will shortly consider a model under which this distinction is possible, albeit at a cost.) But the point of this model is that the platform cannot distinguish the two. A choice of $\hat{x}$ is a choice about the acceptable ratio of babies to bathwater.

The rest of the analysis is an equally straightforward recapitulation of the previous model. As before, the socially optimal level of moderation $x^e$ is defined by the point at which the social-benefit curve $p(x)$ and the expected-harm curve $H\lambda(x)$ intersect. At this point, the additional harms from the fraction of content that is harmful exactly cancel out the profits and spillovers from all of the content, harmful and harmless. Put algebraically, a regulator should want the platform to set $\hat{x}$ such that $\lambda(\hat{x}) = \frac{s(\hat{x})}{H}$: the greater the harm $H$, the lower the probability $\lambda(x)$ of harm worth tolerating, and thus the lower the appropriate threshold of moderation.

The platform, however, does not have an incentive to set its moderation threshold at the socially optimal level. Instead, as above, its profit-maximizing strategy, is to set the threshold $\hat{x}$ at the point where its marginal profit is exactly zero, i.e. where $p(x) = 0$. Thus $x^* > x^e$ and the platform undermoderates.

And finally, as above, strict liability restores efficiency. Note that for the fraction $1 - \lambda(x)$ of content that is actually harmless, the platform pays no damages. But for the fraction $\lambda(x)$ of content that is harmful, the platform pays the full $H$, for total damages of $H\lambda(x)$. The platform's profit under strict liability becomes $p(x) - H\lambda(x)$, which is equal to social welfare, so it maximizes its profits by setting its moderation threshold to $x^e$, and society is as well-off as it can be, given the indistinguishability of harmful and harmless content.


\subsection{Positive Externalities}

The second essential feature of arguments for platform immunity is that the content platforms carry systematically has positive spillovers for society. A widget factory might come close to capturing the full social value of the widgets it makes. But a platform does not, for at least two reasons.

First, a platform's ``product'' is often not widgets but speech. Speech consists of information, and information is a public good. Once it has been shared with one listener, the speaker cannot easily prevent them from sharing it with others.\footnote{Arrow, Lemley, Frischmann, Baker} A dance video that goes viral on TikTok will be reposted to Twitter and YouTube; the information in a plumbing tutorial will be retained in the minds of viewers and shared with others. All of this third-party value is an externality from the speaker's perspective.

The second source of positive externalities is that platforms do not even capture the full value to speakers of the content they host. As Felix Wu convincingly argues, the value to a user of \emph{posting} content to a platform is typically much larger than the value to the platform of \emph{hosting} that content. A platform does not have an original speaker's incentives. And this point holds true even for non-speech platforms: Airbnb captures only part of the value that apartment hosts extract from rentals made through the platform.

We model these positive externalities by introducing a function $s(x)$, which captures the total value to society of the platform's hosting the $x$-th unit of content. We assume that the ordering of content from best to worst is the same for the platform and for society -- so that $s(x)$ is also weakly decreasing. We also assume that this spillover value is never negative, so that $s(x) \ge p(x)$. (Any negative externalities are already accounted for in by the harm function $H\lambda(x)$.)

\autoref{fig:full1} illustrates the consequences for social welfare when the platform sets its moderation level to $\hat{x}$ in the presence of positive externalities. The green checked area between $p(x)$ and the $x$ axis is the platform's revenue from the content it carries; the blue dotted area between $s(x)$ and $p(x)$ is the additional positive spillovers for society. The red striped area under $H\lambda(x)$ is the harms due to the content the platform carries.

\autoref{fig:full1} also illustrates that neither blanket immunity nor strict liability is necessarily efficient. Under blanket immunity, the platform sets its moderation level to $\xbi$, where $p(x) =0 $. Here, this results in undermoderation, because the platform fails to take into account the harms from the content it carries. But under strict liability, the platform sets its moderation level to $\xsl$, where $p(x) = H\lambda(x)$ and its revenues from carrying additional are exactly cancelled out by the harm that content causes (and hence the damages it must pay). Here, this results in overmoderation, because the platform fails to take into account the spillover benefits from the content it carries.

\begin{pgfecon}{Social welfare in the presence of positive externalities}{fig:full1}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{4.25}{1.625}{$\xsl$}
  \dropline{5.1}{2.6}{$x^e$};
  \dropline{6.5}{4}{$\hat{x}$};
  \draw[dashed, thin] (8, 0) -- (8, -.5) node[below]{$\xbi$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:6.5}];
  \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:6.5}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:6.5}];

 % \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:3.5}];
 % \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:3.5}];
\end{pgfecon}

This is the fundamental challenge of platform liability law. Content has both harms and benefits to society that the platform does not internalize. A profit-maximizing platform makes its decisions based on how much it can make from hosting content, paying no attention to either positive or negative spillovers. Whether this results in overmoderation or undermoderation depends on the exact balance of harms and benefits. Liability and other regulations can change the platform's calculus. But there is no simple solution that automatically lines up the platform's incentives with society's.


\subsection{Platform Investigations}

The final moving piece of our model is that a platform can investigate content that it suspects of being harmful. Specifically, the platform can pay a cost $c \ge 0$  per unit of content to investigate and determine with certain whether each item is actually harmful.

To get intuition for how this possibility affects the platform's incentives, start with extreme cases. When investigation is infeasibly costly to ever undertake, i.e. $c \to \inf$, this model collapses into the previous one, because there are no circumstances under which the option to investigate is worth exercising.

On the other hand, when investigation is costless, i.e. $c \to 0$, the platform can  perfectly distinguish harmful content and harmless content. That means it is possible for the platform to take down the harmful content while still leaving up the harmless content. From the regulator's perspective, that is exactly what it should do: take down every piece of harmful content and leave up every every piece of harmless content. Under these circumstances, strict liability for the harms actually caused is again efficient. It is no longer a problem of imperfect information, and the chilling-effect argument for intermediary immunity vanishes.

Now consider what happens for intermediate $c$. The platform has three options for any given item of content: it can leave it up, take it down, or investigate. It is easy to see that the platform will only investigate content where its decision depends on the results of the investigation -- i.e.,  it will take the content down if the investigation reveals it to be harmful, and leave it up otherwise. (If the platform intended to take down the content regardless, it could save $c$ by omitting the investigation, and similarly if it intended to leave up the content regardless.)

Thus the expected value to society for content at $x$ is $0$ if the platform takes down the content, $s(x) - H\lambda(x)$ per unit if it leaves the content up, and $s(x)(1 - \lambda(x)) - c$ per unit if it investigates -- i.e., the value of a harmless piece of content times the probability that the content is harmless. Intuitively, the platform should prefer takedown for content with $\lambda(x)$ close to $1$ and should prefer leaving up for content with $\lambda(x)$ close to $0$, with an interval of investigation somewhere in the middle. 

\begin{pgfecon}{Investigation of intermediate content}{fig:investigate1}
  \lambdaline
  \plotline{harmline}{5}{$H$}
  \plotline{hcline}{4.5}{$H - c$}
  \plotline{cline}{.5}{$c$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{3.55}{1.73}{$\xsl$}
  \dropline{5.2}{2.6}{$x^e$};
  \plotpartialvalue{2}{20}{0}{1.66}{green};
  \plotpartialvalue{2}{20}{1.66}{5.63}{yellow};
  \plotpartialvalue{2}{20}{5.63}{10}{red};
  \plotpartialvalue{3.5}{15}{0}{2.99}{green};
  \plotpartialvalue{3.5}{15}{2.99}{6.87}{yellow};
  \plotpartialvalue{3.5}{15}{6.87}{10}{red};
  
  \draw[domain = .9:10, samples=200, name path = lowerlimit] plot (\x,{5- 5/\x});
  \draw (1, -.5) -- (1,-.5) node[below]{lower limit};
  \draw[domain = 0:9.1, samples=200, name path = upperlimit] plot (\x,{5 /(10 -\x)}) node[right]{upper limit};
  
  \addplot [pattern= dots, pattern color = green] fill between [of = harmline and lambda, soft clip={domain=0:1}];
  \addplot [pattern= dots, pattern color = green] fill between [of = harmline and lowerlimit, soft clip={domain=1:9}];
  \addplot [pattern= dots, pattern color = green] fill between [of = harmline and lambda, soft clip={domain=9:10}];
  
  \addplot [pattern= dots, pattern color = yellow] fill between [of = upperlimit and lowerlimit, soft clip={domain=1.1:9}];
  
  \addplot [pattern= dots, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:1}];
  \addplot [pattern= dots, pattern color = red] fill between [of = upperlimit and axis, soft clip={domain=1:9}];
  \addplot [pattern= dots, pattern color = red] fill between [of = lambda and axis, soft clip={domain=9:10}];
  
\end{pgfecon}

The regulator is indifferent between takedown and investigation when (1) the value of the content that investigation will allow to remain up minus the costs of investigation exactly equals (2) the value of taking all content down. (1) consists of the social value $s(x)$ times the fraction of harmless content $1 - lambda(x)$,  minus minus $c$ (times $1$, as the platform must investigate all content, harmful and harmless). (2) is simply $0$. Doing out the math, takedown and investigation are equally efficient when 
 \begin{equation*}\lambda(x) = 1 - \frac{c}{s(x)}.\end{equation*} When $c$ approaches $0$, this converges to $\lambda(x) = 1$, i.e., so the right end of the investigation interval approaches $\xmax$. That is, as the costs of investigation decrease, it is almost always better to investigate than to take down suspected-bad content without first checking. 

The regulator is indifferent between investigation and leaving up when (1) the value of the content that investigation will allow to remain up minus the costs of investigation exactly equals (2) the benefits of all the content minus the costs of the harmful content. (1) is the same as before: $s(x)(1 - \lambda(x)) - c$. (2) simply consists of the social benefits $s(x)$ minus the harms $H\lambda(x)$. Doing out the math, investigating and leaving up are equally efficient when 
\begin{equation*}\lambda(x) = \frac{c}{H - s(x)}.\end{equation*} When $c$ approaches $0$, this converges to $\lambda(x) = 0$, i.e., the left end of the investigation interval approaches $0$. That is, as $c$ decreases, it almost always better to investigate than to leave up the suspected-good content without first checking. 

That is, as $c$ decreases, the ideal investigation interval expands to cover more and more content. On the other hand, for increasing $c$, the investigation interval shrinks and eventually vanishes. \footnote{It vanishes when:
\begin{equation*}
c > \min_{x \in [0, \xmax]} s(x)\frac{H - s(x)}{2s(x) - H}.
\end{equation*}}
When this bound is exceeded, it is never worthwhile from society's perspective for the platform to investigate. It should instead, as in the previous model, act on the basis of the imperfect information it already has.

These results show that a rational regulator should want platforms to invest resources in investigating only when the costs of investigation are sufficiently low, and then only for a range of intermediate cases where the harmfulness of the content is sufficiently unclear. For content that is highly likely or highly unlikely to be harmful, individual investigation is unnecessary and inefficient. Note that this interval contains $x^e$ -- in a sense, affordable investigations expands the cutoff from a sharp on-off to a range warranting a closer look.
% \footnote{In notation, the efficient range of investigation is  \begin{equation*}[\frac{c}{H - s(x)}, 1 - \frac{c}{s(x)}].\end{equation*}}

\autoref{fig:investigate1} illustrates.\footnote{For simplicity of illustration, $\lambda(x)$ is shown as a straight line, but the same results hold in the general case where it is any weakly increasing function that goes from $0$ to $1$ on the interval $[0,\xmax]$.} The curve labeled ``lower limit'' is the dividing line between the region where investigation is better than leaving content up and vice versa. The curve labeled ``upper limit'' is the dividing line between the region where investigation is better than taking content down, and vice versa. These are two-dimensional regions, because whether it is rational to investigate or not depends both on $lambda(x)$ (the horizontal axis) and on $s(x)$ (the vertical axis). As the probability of content being harmful increases (i.e., as one moves horizontally to the right), one starts in a region where it is optimal to leave content up, passes through a region (possibly zero-width) where investigation is optimal, and then moves into a region where it is optimal to take content down. Similarly, as the value of content increases (i.e. as one moves vertically upwards), the optimal policy changes from takedown to investigation to leaving content up. If the curve $s(x)$ passes through the investigation-justified region at all, then $x^e$ lies within it.

\autoref{fig:investigate1} also illustrates the dependence of investigation on $c$. As $c$ decreases, the upper limit moves upwards and the lower limit moves downwards, increasing the size of the (yellow dotted) region where investigation is justified. As $c$ increases, the limits converge, until eventually the region vanishes entirely. In this case, investigation is never justified and we are back to the previous model, where $H\lambda(x)$ marks the dividing line between taking down and leaving up.

A very similar analysis applies to a platform's incentives under strict liability.\footnote{Under blanket immunity, a platform will never investigate. Instead, it will always choose to leave all content up.} Because the platform internalizes all the harm that it causes, the only change is to substitute the platform's private profit $p(x)$ for the overall social value $s(x)$. If there is any range for which investigation is justified, it will contain  $\xsl$.) % \footnote{Its interval of investigation is \begin{equation*}[\frac{c}{H - p(x)}, 1 - \frac{c}{p(x)}]\end{equation*}} 
A little algebraic manipulation shows that the platform's preferred interval of investigation is always \emph{shifted left} from the regulator's preferred interval.\footnote{To be precise, at the lower end \begin{equation*}\frac{c}{H - s(x)} < \frac{c}{H - p(x)},\end{equation*} and at the upper end \begin{equation*}1 - \frac{c}{p(x)} < 1 - \frac{c}{s(x)}.\end{equation*}} Intuitively, because the platform has less at stake, it will be more likely to remove content rather than investigating and also more likely to investigate content rather than leaving it up.


\section{Platform Liability and Platform Immunity}

We have already seen that neither blanket immunity nor strict liability is guaranteed to be efficient. This Part provides a comparative analysis of the arguments for and against other liability regimes. The point is not to definitely settle on one or another as optimal, but instead to bring out the intuitions behind each and to get a sense of the conditions they depend on.

strict liability

negligence
conditional immunity

blanket immunity

must-carry (for non-harmful content)
must-carry (absolute)

(pre-230) must-carry OR strict liability

Section 230
  and reforms thereof

Section 512

DMA


\begin{itemize}
\item \textbf{Blanket Immunity}: If the factory faces no liability for pollution, it will produce until its marginal revenue from each additional widget is zero -- i.e., until $p(x) = 0$.\footnote{If the factory is not profitable at this point, there is no level of production at which it can possibly be profitable.} But if there are any pollution harms at all at this level of production, this point will be to the right of $x^e$, and social welfare will be less than the efficient level. If $h(x)$ is high enough, social welfare may even be negative.
\item \textbf{Prohibition}: If society bans widgets, it compels the factory to set $\hat{x} = 0$ and make a profit of $0$. But since $0 < x^e$, social welfare will be less than the efficient level. For some values of $h$ and $p$, prohibition will be better than no liability; for others, it will be worse.
\item \textbf{Must-Carry}: Society could also compel the factory to maximize production by setting $\hat{x} = n$. This regime is also inefficient. Indeed, it is even worse than blanket immunity in cases where the factory's marginal profit $p(x)$ goes negative for high enough production -- the factory is affirmatively compelled to produce unprofitable widgets. 
\item \textbf{Negligence}: The regulator sets a threshold $t$. The platform faces no liability for content below that threshold, but is liable for the harms caused by production above that threshold. It is a standard law-and-economics result that setting $t = x^e$ gives the platform the same incentives as strict liability. Setting $t < x^e$ also produces the right marginal incentives at $x^e$, but can cause the factory to fail in some cases where it cannot cover its fixed costs $F$.
\item \textbf{Reasonable Efforts}: The regulator sets a harm threshold $T$. The factory faces no liability as long as the total harms it causes are beneath this threshold, i.e. $\int_0^{\hat{x}} h(x) dx \le T$. But if the harm exceeds this threshold, the factory loses its immunity and is liable for \emph{all} the harms it caused, even those beneath the threshold. If the regulator sets $T = \int_0^{t} h(x)$, then the factory sets $\hat{x} = x^e$ and acts efficiently. As above, setting $T$ beneath this value also produces the right marginal incentives at $x^e$, but can cause the factory to fail if it cannot cover its fixed costs.
\end{itemize}


\begin{econ}{Blanket immunity}{fig:liability2}
  \drawaxes{production}{\$}
  \harmfunction{harm}
  \dropline{5}{2.5}{$\hat{x}$}
\end{econ}

% \begin{econ}{Prohibition}{fig:liability3}
%   \drawaxes{production}{\$}
%   \harmfunction{harm}
%   \dropline{0}{0}{$\hat{x}$}
%   \fill[pattern=vertical lines, pattern color=red] (0,0) to (10,0) to (10,5.2) to (0,5.2) to (0,0);
% \end{econ}

\begin{econ}{Negligence}{fig:liability4}
  \drawaxes{production}{\$}
  \harmfunction{harm}
  \dropline{5}{2.5}{$t$}
  \dropline{7}{4.35}{$\hat{x}$}
  \fill[pattern=vertical lines, pattern color=red] (5,2.5) parabola bend (9,5) (7,4.35) to (7,0) to (5,0) to (5,2.5);
\end{econ}

\begin{econ}{Conditional immunity (below threshold)}{fig:liability5}
  \drawaxes{production}{\$}
  \harmfunction{harm}
  \dropline{5}{2.5}{$t$}
  \dropline{4.1}{1.5}{$\hat{x}$}
\end{econ}

\begin{econ}{Conditional immunity (above threshold)}{fig:liability6}
  \drawaxes{production}{\$}
  \harmfunction{harm}
  \dropline{5}{2.5}{$t$}
  \dropline{5.9}{3.5}{$\hat{x}$}
  \fill[pattern=vertical lines, pattern color=red] (1,0) parabola (5, 2.5) parabola bend (9,5) (5.9, 3.5) to (5.9, 0) to (1, 0);
\end{econ}


\subsection{Strict Liability}





\subsection{Blanket Immunity}


Figure \ref{fig:harm2} shows that blanket immunity can lead to undermoderation: the platform makes money from hosting content that is, on average, a net negative for society. In this region between $x^e$ and $x^*$ (red and striped), there are both harmful and harmless items of content, but the harm from the harmful items outweighs the benefits of all the content together. Making the platform liable for the harmful content it carries could potentially improve matters, by encouraging it to carry less content.

\begin{pgfecon}{Blanket immunity can lead to undermoderation}{fig:harm2}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \dropline{5.1}{2.6}{$x^e$};
  \draw[dashed, thin] (8, 4.75) -- (8, -0.5) node[below]{$x^*$};
  %\draw[dashed, thin] (9.25, 0) -- (9.25, 0.5) node[above]{$x^e$};
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=0:5.1}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and welfare, soft clip={domain=5.1:8}];

 % \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:3.5}];
 % \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:3.5}];
\end{pgfecon}

Figure \ref{fig:harm3} shows that even under blanket immunity, a platform might still overmoderate. As above, if the platform's profits are small enough compared with the social value of the content it carries, it may simply not be economically feasible for the platform to carry all the content it would be socially valuable to. Making the platform liable for harmful content it carries would only make matters worse, by causing it to carry even less content. 

\subsection{Blanket Immunity}


When the platform overmoderates in the presence of harm, a must-carry regime could potentially improve matters by causing it to carry more content. But it would need to satisfy the same two kinds of conditions as above. First, the harm due to the additional content it carries must not outweigh the value of the additional content -- that is, the signed area between $s(x)$ and $H\lambda(x)$  from $x^e$ to $\xmax$ but be positive. Second, it must not force the platform out of business -- that is, the signed area between $p(x)$ and the $x$ axis from $0$ to $\xmax$ must also be positive.

\begin{pgfecon}{Blanket immunity can lead to overmoderation}{fig:harm3}
  \lambdaplot
  \plotline{harmline}{5}{$H$}
  \plotvalue{profit}{.5}{25}{$p(x)$}
  \plotvalue{welfare}{4.5}{50}{$s(x)$}
  \dropline{5.1}{4.25}{$x^*$};
  \draw[dashed, thin] (6.5, 4) -- (6.5, 0) node[above right]{$x^e$};
  %\draw[dashed, thin] (8, 0) node[below left]{$x^*=x^e$};
  % \addplot [pattern= dots, pattern color = blue] fill between [of = profit and welfare, soft clip={domain=0:4.28}];
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and lambda, soft clip={domain=0:5.1}];
  \addplot [pattern= vertical lines, pattern color = cyan] fill between [of = lambda and welfare, soft clip={domain=5.1:6.5}];

 % \addplot [pattern= dots, pattern color = blue] fill between [of = welfare and profit, soft clip={domain=0:3.5}];
 % \addplot [pattern= north east lines, pattern color = red] fill between [of = lambda and axis, soft clip={domain=0:3.5}];
\end{pgfecon}

Return now to the case of undermoderation, where the platform leaves up content that is socially harmful but privately profitable. T



There are several ways that a regulator could attempt to fix the platform's misaligned incentives, many of which are familiar from telecommunications and intellectual-property law. For example, the regulator could directly subsidize the platform in the region between $x^*$ and $x^e$, paying it to host content it otherwise would not. Or it could subsidize the users who create that content, allowing them to increase the price they pay the platform to host the content.

More relevantly, a regulator could also impose a \emph{must-carry} rule, in which the platform must host all content submitted to it. (Visually, this amounts to i.e. forcing the platform to set $\hat{x}$ to the far right edge of the diagram.) In the case where $s(x)$ never goes negative, must-carry is obviously efficient, because every hosted item of content adds to social welfare.

There can even be an argument for must-carry even when it results in hosting some negative-value content. The logic here is that a flat must-carry system might be substantially easier to implement than trying to nail down $x^e$ precisely. Something like this, for example, might be an argument for network neutrality: the game of investigating whether content an ISP wants to block is socially harmful is not worth the candle.

A must-carry rule, however, must satisfy two conditions to be justified. First, it must actually result in hosting more worthwhile than worthless content. In Figure \ref{fig:mustcarry1}, the green gridded region is the positive-value content that must-carry causes to be hosted, and the red striped region is the negative-value content it also causes to be hosted. If the red region is larger than the green one, must-carry is counter-productive; the bad additional content outweighs the good.

\begin{pgfecon}{Must-carry from society's point of view}{fig:mustcarry1}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \draw[dashed, thin] (8, .75) -- (8, -0.5) node[below]{$x^*$};
  \draw[dashed, thin] (9.25, 0) -- (9.25, 0.5) node[above]{$x^e$};
  \addplot [pattern= grid, pattern color = green] fill between [of = welfare and axis, soft clip={domain=8:9.25}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = axis and welfare, soft clip={domain=9.25:10}];
\end{pgfecon}

A little more subtly, must-carry can also counter-productively drive a platform out of the market. In Figure \ref{fig:mustcarry2}, the green gridded region is the profit's profits from hosting the content it wants to, and the red striped region it is losses from hosting the content it is forced to. If the red region is larger than the green one, it is unprofitable for the platform to operate at all, and it will rationally shut down rather than comply with a must-carry mandate.

\begin{pgfecon}{Must-carry from the platform's point of view}{fig:mustcarry2}
  \plotvalue{welfare}{3.5}{15}{$s(x)$}
  \plotvalue{profit}{2}{20}{$p(x)$}
  \draw[dashed, thin] (8, .75) -- (8, -0.5) node[below]{$x^*$};
  \draw[dashed, thin] (9.25, 0) -- (9.25, 0.5) node[above]{$x^e$};
  \addplot [pattern= grid, pattern color = green] fill between [of = profit and axis, soft clip={domain=0:8}];
  \addplot [pattern= north east lines, pattern color = red] fill between [of = axis and profit, soft clip={domain=8:10}];
\end{pgfecon}





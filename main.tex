\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}

\title{An Economic Model of Section 230}
\author{James Grimmelmann}
\date{June 2021}

\begin{document}





\maketitle

\section{A Basic Model}

A platform receives $n > 0$ items of user-submitted content. It has the option of either operating and receiving a revenue of $R > 0$ per item, or shutting down and receiving a revenue of $0$. Since its revenues are always positive, it will always prefer to operate, with profit $nR$.

From society's perspective, a fraction $0 \le \gamma \le 1$ of the items are ``good'' content with social benefit $G > 0$, and a fraction $0 \le \beta \le 1$  are ''bad'' content with social cost $B > 0$. Since every piece of content is one or the other, $\gamma + \beta = 1$. If the platform shuts down, society's total welfare is $0$, but if it operates, its welfare is the sum of $nG\gamma$ for the good content and $-nB\beta$ for the bad content, i.e.
\begin{equation*}
nG\gamma - nB\beta
\end{equation*}
Society is better off with the platform as long as the benefits from good content exceed the harms from bad content. This is the case as long as
\begin{equation}
\label{shutdown1}
\frac{\gamma}{\beta} > \frac{B}{G} 
\end{equation}
In other words, there must be enough good content as compared with bad to outweigh the cost of bad content as compared with good. If there is, society should tolerate the platform; if there is not, society should force the platform to shut down.

\section{Liability}

Shutting down a platform is easier said than done. There are serious Constitutional barriers to directly prohibiting a platform, including especially the First Amendment. Regulators may also not be confident in their ability to assess the overall balance of costs and benefits from the content on the platform.

For these reasons, it is more feasible to impose liability on platforms for the bad, i.e. illegal, content that they carry. Rather than being prohbiited entirely from operating, they are penalized if and only if they host content that society has chosen to prohibit. 

We model this by adding a liability term to the platform's profits. It now must pay $L$ per item of bad content that it leaves up. Society's welfare does not change under the platform's two options (operate and shut down), but the platform's incentives do. If the platform shuts down, its profit remains $0$.  If it operates and allows all content to stay up, its  profit is now 
\begin{equation}
\label{profnofilter}
nR - nL\beta
\end{equation}
This changes the platform's incentives. It will shut down if its profit from operating is negative, i.e., when $nR - nL\beta < 0$. Rearranging and cancelling out the factor of $n$, this means that for society to shut the platform down, it must set the liability $L$ for each item of bad content such that
\begin{equation*}
L > R\frac{1}{\beta}
\end{equation*}
This inequality has several implications. First, since $\beta$ is a fraction always less than or equal to $1$, the liability $L$ per item of bad content must always be equal to or greater than the platform's revenue $R$ from that item of content. Otherwise, the platform will simply take the hit and allow the content to stay up, making positive profit of $R -L$ per bad item.

How much greater $L$ must be than $R$ depends on $\beta$. In the limit when $\beta \rightarrow 1$ (i.e., almost all of the content is bad), $L$ approaches $R$. But in the limit when $\beta \rightarrow 0$ (i.e. almost all of the content is good), $L$ approaches infinity! This may seem counterintuitive --- why should more liability be required when less of the content is bad --- but it makes sense given that the platform's options are operating or shutting down. To outweigh the revenue the platform receives from the mostly good content, the liability per item on the small fraction of bad content must be large. And when $\beta$ is small, shutdown-inducing liability will only be socially justified when $B \gg G$, i.e., when the harms from each piece of bad content are especially severe.

\section{Filtering}

Even better that shutting down a platform entirely merely because it has some bad content on it would be distinguishing the good content from the bad and removing the bad content while leaving up the good.  So ideally, from society's perspective, would be for platforms to implement filters that can distinguish wheat, sheep, and babies from chaff, goats, and bathwater.

We model filtering by giving the platform a new option. It can distinguish between good and bad content at a cost of $C > 0$ per item. It must forego the revenue $R$ from any item of content that it removes, but otherwise blocking is costless. 

Now the platform has three options instead of two. As before, it has the option of allowing all content to stay up for a total profit of $nR - nL\beta$. As before, it has the option of shutting down for a profit of $0$. It will still choose between them according to equation~\ref{shutdown1}, shutting down if $L$ is too high.

The platform's new option is to pay $nC$ to examine every item of content and then remove the bad ones, foregoing $nR\beta$ in revenue. This makes its total profit
\begin{equation}
\label{proffilter}
nR\gamma - nC    
\end{equation}
Note that it is never rational for the platform to pay to examine content and then leave the bad items up. In the case where $R > L$, the platform is better off not paying $C$ at all and leaving the bad items up without even looking at them. So there are three options -- no filtering, filtering, and shutdown -- and not four.

The platform will now prefer to filter (rather than leaving all content up) if its profit with filtering $nR\gamma - nC$ (equation \ref{proffilter}) exceeds its profit without filtering but with liability $nR - nL\beta$ (equation \ref{profnofilter}). Rearranging, this condition is equivalent to 
\begin{equation*}
\label{dofilter}
L \;>\; R + C\frac{1}{\beta}
\end{equation*}
That is, the liability $L$ for each item of bad content must not only exceed the platform's revenue $R$ for that item, but also the cost of filtering. That cost is not just $C$, the cost to inspect one item, since the platform must inspect \emph{every} item in order to find the bad ones. This filtering cost per bad item increases as the proportion of bad items decreases. If one half of all items are bad, then the  liability must be greater than $R + 2C$. If one millionth of all items are bad, the liability must be greater than $R + 1,000,000C$. Thus, when the prevalence of bad content is sufficiently low, a rational platform  will prefer to take the occasional liability hit rather than spend exorbitant sums searching for needles in haystacks.

This might suggest that society should simply increase the liability $L$
per bad item to the point at which a platform prefers to filter. But the platform's other option also functions as a constraint. If filtering is too expensive, the platform will prefer to shut down rather than bother. This constraint kicks in unless $nR\gamma - nC > 0$, i.e. when 
\begin{equation}
\label{shutdown2}
R \;>\; C\frac{1}{1 - \beta}
\end{equation}
In words, unless the platform's revenue for each item of good content (all that it can leave up with filtering) exceeds its expected cost of filtering per item of good content, it is better off shutting down than filtering. This time the math of low-prevalence bad items cuts the other way, albeit not as strongly. If half of the items are bad, then its revenue must be greater than $2C$ for filtering to beat shutting down. But if one millionth of all items are bad, the platform's revenue per good item must be greater than about $1.000001C$. Notice that this value approaches from above a limit of $C$ as the prevalence of bad content decreases, and can never go below $C$. In the limit, if the platform's cost to filter per item exceeds its revenue per item --- if $C > R$ --- then it is never worthwhile for the platform to filter. It is better off just shutting down.

These two constraints together mean that a high enough level of liability can force a platform to shut down.  Society would prefer that the platform filter rather than leaving all content up, and it can raise the liability $L$ high enough to make the platform prefer to filter, too. But if the platform also prefers to shut down rather than filter, because filtering would push its profits below zero, that level of liability will induce the platform to shut down. The threat of liability only works to induce filtering if it leaves the platform with a profitable option. 

What society ought to do, then, depends on the relative magnitudes of $\gamma$, $\beta$, $R$, $C$, $G$, and $B$. Filtering is a feasible option as long as $R > C/(1 - \beta)$. If so, then any level of liability $L > R + C/\beta$ will cause the platform to shift from leaving all content up to filtering. The actual level of liability is irrelevant, because the platform, having chosen to filter, never actually incurs liability.  If filtering is unprofitable for the platform, then society must choose between no filtering and shutdown per equation \ref{shutdown1}. 

Putting this all together, the case for intermediary immunity is justified is when $C > R\gamma$ and $\gamma/\beta > B/G$. Effective filtering is cost-prohibitive, so that imposing liability will lead the platform to overcensor (from society's point of view) by shutting down.

\end{document}

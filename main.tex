\documentclass[openbib,12pt]{article}  % Comments shown
\usepackage{amsmath,amssymb,amsthm,mathabx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{arydshln}
\usepackage{natbib}
%\usepackage[utf8x]{inputenc}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage[margin=1in]{geometry}
\usepackage{mathpazo}
\usepackage{dsfont}
\usepackage[multiple]{footmisc}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage[colorlinks,citecolor=blue]{hyperref}
\usepackage{lscape}
\usepackage[hang,centerlast]{caption}
\usepackage{makecell}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable}
\usepackage[pdftex]{color}
\usepackage[table]{xcolor}
\usepackage{rotating}
\usepackage{url}
\usepackage{tikz}
\usepackage{pgfplots}
% We will externalize the figures
%\usepgfplotslibrary{external}
%\tikzexternalize
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{intersections}%, pgfplots.fillbetween}
\usetikzlibrary{patterns}
\usepgfplotslibrary{fillbetween}
\usepackage{subfigure}
\usepackage[capposition=bottom]{floatrow}
\usepackage[all]{xy}
\usepackage[english]{babel}
\usepackage{adjustbox}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}
\DeclareGraphicsRule{*}{mps}{*}{}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{result}{Result}
\newtheorem{finding}{Finding}
\newtheorem{subfinding}{Finding}
\renewcommand{\thesubfinding}{\thefinding\alph{subfinding}}
\newtheorem{subhypothesis}{Hypothesis}
\renewcommand{\thesubhypothesis}{\thehypothesis\alph{subhypothesis}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\plim}{\mathrm{plim}}
\newcommand{\tred}{\textcolor{red}}
\newcommand{\lmt}{\rightarrow}
\newcommand{\lmp}{\Rightarrow}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\nn}{\nonumber}
\newcommand{\gs}{\geqslant}
\newcommand{\ls}{\leqslant}
%% specific to this paper

\renewcommand{\baselinestretch}{1.4} 
\renewcommand{\vec}[1]{\mathbf{#1}}
\oddsidemargin=0truein
\evensidemargin=0truein
\topmargin=0truein\headheight=0truein\headsep=0truein
\textheight=9truein\textwidth=6.5truein
\parskip=12pt
\parindent=1cm
\baselineskip=25pt

\title{An Economic Model of Section 230}
\author{James Grimmelmann}
\date{June 2021}

\begin{document}





\maketitle

\section{An Economic Model of Intermediary Liability}

A platform receives $n > 0$ items of user-submitted content. Of them, a fraction $0 \le \beta \le 1$  are ``bad'' content, each item of which imposes a harm $B \ge 0$ on society. The remaining fraction $1 - \beta$ consists of ``good'' content, each item of which creates a benefit $G \ge 0$ for society. The platform receives a revenue of $R \ge 0$ for each item it carries, good or bad. It can tell the difference between the two types at a cost of $C \ge 0$ per item. If it decides to remove an item of content, it gives up the revenue associated with that item.

A regulator would like to maximize the social benefit from the content carried on the platform. It cannot directly control which items are carried and which are removed. Instead, it can impose a liability $L \ge 0$ on the platform for each item of bad content the platform carries. The regulator's job is to choose a value of $L$ that will induce the platform to maximize the social benefit.

The platform has three viable strategies:
\begin{itemize}

\item  \emph{First}, the platform can carry all content, good and bad, and make no attempt to tell the two apart. If it does so, it receives $nR$ in revenue (for all the content) but pays $n\beta L$ in liability (for the bad content) for a net profit of
\begin{equation}
\label{profitnofilter}
nR - n\beta L
\end{equation}
If the platform carries all content, society realizes a total benefit of $n(1-\beta)G$ from the good items, but a harm of $n\beta B$ from the bad items. Overall social welfare is thus
\begin{equation}
\label{welfarenofilter}
n(1-\beta)G - n \beta B
\end{equation}

\item \emph{Second}, the platform can pay $nC$ to inspect every item of content to discover whether it is good or bad. Once it has done so, it carries the good items for a total of $n(1-\beta)R$ in revenue, but removes the bad items (forgoing $n\beta R$ in revenue). Its net profit is therefore 
\begin{equation}
\label{profitfilter}
n(1 - \beta)R - nC    
\end{equation}
If the platform filters, society still realizes the benefit of  $n(1-\beta)G$ from the good items of content, but no harm because the bad items have been removed. Social welfare is
\begin{equation}
\label{welfarefilter}
n(1-\beta)G
\end{equation}

\item \emph{Third}, the platform can shut down and carry no content. It receives no revenue, but incurs no liability, so its net profit is simply
\begin{equation}
\label{profitshutdown}
0  
\end{equation}
Similarly, society receives no benefit from the good content and harms from the bad content, so social welfare is also simply 
\begin{equation}
\label{welfareshutdown}
0  
\end{equation}
\end{itemize}

\section{The Regulator's Goals}

Filtering (equation~\ref{welfarefilter}) is always the regulator's preferred outcome. It is better than no filtering (equation~\ref{welfarenofilter}) as long as there is any bad content ($\beta > 0$) and that content causes harm ($B > 0$). It is better than shutdown (equation~\ref{welfareshutdown}) as long as there is any good content ($\beta < 1$) and that content creates benefits ($G > 0$).

The regulator's second choice, however, depends on the prevalence $\beta$ of bad content. Social welfare is higher with no filtering than with shutdown as long as $n(1-\beta)G - n \beta B > 0$. In words, an unfiltered platform does more good than harm as long as there is enough good content as compared with bad to outweigh the cost of bad content as compared with good. Rearranging and solving for $\beta$, this indicates that the regulator should prefer to have the platform operate as long as
\begin{equation}
\label{welfarefvs}
\beta \;<\; \frac{G}{B+G}
\end{equation}
When $B=G$, i.e. the cost of an item of bad content exactly equals the benefit of an item of good content, equation~\ref{welfarefvs} indicates that $\beta < \frac{1}{2}$, i.e. there must be less bad content then good. When $B \gg G$, i.e., each item of bad content causes much more harm than each item of good content creates benefit, $\beta$ approaches $0$, i.e., there must be much less bad content than good. When $B \ll G$, i.e. each item of bad content causes much less harm than each item of good content creates benefit, $\beta$ approaches $1$, i.e., there can be much more bad content than good.

\section{The Platform's Incentives}

Now consider things from the platform's perspective.  Its choices depend on the value of $L$. The platform will prefer to filter (rather than leaving all content up) if its profit with filtering (equation \ref{profitfilter}) exceeds its profit without filtering but with liability (equation \ref{profitnofilter}), i.e. if $nR(1 - \beta) - nC >  nR - nL\beta$. Rearranging and solving for $L$,  this condition is equivalent to 
\begin{equation}
\label{dofilter}
L \;>\; R + C\frac{1}{\beta}
\end{equation}
That is, the liability $L$ for each item of bad content must not only exceed the platform's revenue $R$ for that item, but also the cost of filtering. That cost is not just $C$, the cost to inspect one item, since the platform must inspect \emph{every} item in order to find the bad ones. This filtering cost per bad item increases as the proportion of bad items decreases. If one half of all items are bad, then the  liability must be greater than $R + 2C$. If one millionth of all items are bad, the liability must be greater than $R + 1,000,000C$. Thus, when the prevalence of bad content is sufficiently low, a rational platform  will prefer to take the occasional liability hit rather than spend exorbitant sums searching for needles in haystacks.  

Since the regulator always prefers filtering to no filtering, it might seem that it should simply set $L$ high enough to induce filtering. Unfortunately, this strategy can backfire, since the platform has a third option open: shutting down. It will operate with filtering only as long as its expected profits with filtering are positive, i.e. when $n(1 - \beta)R - nC > 0$. Solving for $C$ gives 
\begin{equation}
\label{doshutdown}
C \;<\; (1 - \beta) R
\end{equation}
I.e., operating with filtering is uneconomical if the revenue from the good items is insufficient to pay for filtering all of the items. For any given $\beta$ and $R$, there is some critical upper limit of $C$ at which the platform cannot break even with filtering. Call this limit $C^*$; it is equal to $(1-\beta) R$. If $L$ has been set so high that the platform is better of filtering than not filtering, the platform is also better off shutting down than not filtering.

That is, if the per-item cost of filtering $C$ is above the critical value $C^*$ there is no value of $L$ through which the regulator can induce the platform to filter. The platform is always better off either leaving all the content up or taking all of it down. The cost of sifting through it to distinguish the good content from the bad is simply too high.

Society would prefer that the platform filter rather than leaving all content up, and it can raise the liability $L$ high enough to make the platform prefer to filter, too. But if the platform also prefers to shut down rather than filter, because filtering would push its profits below zero, that level of liability will induce the platform to shut down. The threat of liability only works to induce filtering if it leaves the platform with a profitable option. 

In this situation where $C > C^*$, the regulator still has a choice to make. It can set $L$ high enough to actually force the platform to shut down, or it can set $L$ low enough to allow the platform to operate and accept that it will not filter out the bad items of content. It should choose based on equation~\ref{welfarefvs}, i.e. based on whether the platform overall is beneficial or harmful.

Putting this all together, the case for intermediary immunity is justified when $C > C^*$ and $\beta < G/(B+G)$ Effective filtering is cost-prohibitive, so that imposing liability will lead the platform to overfilter (from society's point of view) by shutting down. But since society prefers to have the platform to not having it (because the good content still outweighs the bad), it is better off with underfiltering than overfiltering.


\section{Extensions}

Only a subset of good and bad content can be confused.

$\alpha$ is unambiguously bad.
$\beta$ is ambiguously bad.
$\gamma$ is ambiguously good.
$\delta$ is unambiguously good.

Filtering costs $L_0 + nL$ where $n$ is the number of items filtered.

The platform has fixed costs $F$, regardless of how many items of content it allows.

Different levels of liability (e.g. loss of safe harbor)

\section{Pengfei's version}

\subsection{Model I}
A platform hosts $x\gs 0$ items of user-generated content. 
The inverse demand curve for the content is $P(x)$, interpreted as the marginal benefit from consuming the $x$-th unit of the content. 
Let $s$ be the expenditures or the value of effort on moderating one item of content.
Given $s$, each item might cause harm $h$ with probability $\pi(s)\in[0,1]$, where $\pi(s)$ is decreasing and convex in $s$. It follows that $\pi(s)hx$ is the expected harm given the amount of content $x$, and $(s+\pi(s)h)x$ is the total cost of moderation and harm, thus the social cost.

Social welfare is given by 
\begin{equation}
    \int_0^x [P(z)-s-\pi(s)h]dz.
\end{equation}
Suppose moderation is a binary choice $s\in\{0,1\}$. Social welfare from moderation is $\int_0^x P(z)dz-\pi(0)hx$. Social welfare without moderation is $\int_0^x P(z)dz-1-\pi(1)hx$. It follows that a social planner prefers moderation if 
\begin{equation}\label{eqn:efficiency_1}
    [\pi(0)-\pi(1)]hx\gs 1,
\end{equation}
where $[\pi(0)-\pi(1)]hx$ is the marginal social benefit from moderation, and $1$ is the marginal cost of moderation. 

The platform is profit-maximizing. Since the prevailing market price is $P(x)$, the profit function is given by 
\begin{equation}
    P(x)x-[s+\pi(s)d]x.
\end{equation}
Profits from moderation is $P(x)x-\pi(0)dx$. Profits without moderation is $P(x)x-1-\pi(1)dx$. Thus, the platform chooses to moderate if 
\begin{equation}\label{eqn:platform_1}
    [\pi(0)-\pi(1)]dx\gs 1.
\end{equation}
Comparing equation \ref{eqn:efficiency_1} and equation \ref{eqn:platform_1}, we have that platform's moderation decision is efficient if $d=h$. That is, strict liability rule on the platform leads to social optimum. 

Notice that the optimality of strict platform liability builds upon the fact that the moderation technology does not \emph{reduce} the amount of content. This is not true for the popular moderation approach of content removal. But it might be true if the moderation remedy is to edit or redact content. Interestingly, under the definition of Section 230, editing or redacting undermines the platform's eligibility for immunity and may hold the platform liable for tortious content. Our economic model shows that this aspect on the scope of Section 230 is in fact consistent with efficiency.

\subsection{Model II}
In the last section, we assume that all content is \emph{ex-ante} homogeneous. We relax that assumption in this section.
There are two types of content $\{G, B\}$ where $G$ stands for the good content, and $B$ stands for the bad content. Let $\lambda$ be the proportion of type $B$ content. The total amount of type $G$ content and type $B$ content are then $(1-\lambda)x$ and $\lambda x$ respectively. 

Suppose a perfect filter is available such that the moderation will remove all the type $B$ content. Social welfare from moderation is $\int_0^{(1-\lambda)x} P(z)dz-1$. Social welfare without moderation is $\int_0^x P(z)dz-\lambda xh$. A social planner prefers moderation if 
\begin{equation}\label{eqn:efficiency_2}
    \lambda xh - \int_{(1-\lambda)x}^x P(z)dz \gs 1,
\end{equation}
where $\lambda xh$ is the expected harm, $\int_{(1-\lambda)x}^x P(z)dz$ is the loss of total surplus because of the reduction in content, and $1$ is the marginal cost of moderation.

The platform's profit from moderation is $P((1-\lambda)x)(1-\lambda)x-1$. The profit with no moderation is $P(x)x-\lambda xd$. The platform prefers moderation if 
\begin{equation}\label{eqn:platform_2}
    \lambda xd - [P(x)x-P((1-\lambda)x)(1-\lambda)x] \gs 1,
\end{equation}
where $P(x)x-P((1-\lambda)x)(1-\lambda)x$ is the change in profits because of the reduction in content. 

Notice that the loss in profit is always smaller than the loss in total surplus since 
\begin{equation*}
    P(x)x-P((1-\lambda)x)(1-\lambda)x \ls P((1-\lambda)x)\lambda x \ls \int_{(1-\lambda)x}^x P(z)dz.
\end{equation*}
Therefore, $d<h$ so that the strict liability is \emph{never} optimal. If $d=h$, the platform will more likely to moderate than desired. 
The optimal liability can be written as 
\begin{equation}\label{eqn:liability_1}
    d=\min \left\{h-\frac{\int_{(1-\lambda)x}^x [P(z)-\hat{P}]dz}{\lambda x}, 0\right\},
\end{equation}
which is the harm minus the normalized per-item loss in consumer surplus conditional on the quantity is non-negative. 

When, if ever, will platform immunity $d=0$ be optimal ? It must be that the first part of equation \ref{eqn:liability_1} is less or equal to zero. And observe that 
\begin{equation*}
    \lambda xh < \int_{(1-\lambda)x}^x [P(z)-\hat{P}]dz < \int_{(1-\lambda)x}^x P(z)dz + 1.
\end{equation*}
Hence, it must be that efficiency requires no moderation. This will be true under one of the two conditions: either (i) the harm $h$ is small, or (ii) the fraction of problematic content $\lambda$ is neither too small nor too large. 

Intuitively, moderation is not socially optimal if the harm is too small or the number of problematic item is too few such that the marginal social benefit cannot justify the cost. Interestingly, when the platform is overwhelmed with problematic content while each of them generates negligible harm, moderation is also sub-optimal because moderation in that case virtually means shutting down a platform with positive consumer welfare (Napster is arguably one such case). 
To see the second point more clearly, the first order derivative of the marginal social benefit (on the left-hand side of equation \ref{eqn:efficiency_2}) with respect to $\lambda$ is $x[h-P((1-\lambda)x)]$, which is positive if $P((1-\lambda)x)<h$ and negative if $P((1-\lambda)x)\gs h$. It follows that as $\lambda$ increases, the social benefit of moderation first goes up and above $1$, reaches its peak, and then goes down and below $1$. 

%%% Adding a graph, regions of liability

\subsection{Model III}
The analysis in the last section assumed no ``false positives''. That is, the moderation technology does not erroneously remove type $G$ content and keep type $B$ content. In reality, such errors happened all the time.\footnote{There are accumulating evidence on over-removal of content by mistakes. See for example, \url{https://cyberlaw.stanford.edu/blog/2021/02/empirical-evidence-over-removal-internet-companies-under-intermediary-liability-laws}}

Let $\pi(s)\in [\frac{1}{2},1]$ be the accuracy of the filter given the expenditure on moderation $s$. If an item is of type $B$, it will be removed with probability $\pi(s)$ and kept with probability $1-\pi(s)$. If an item is of type $G$, it will be kept with probability $\pi(s)$ and removed with probability $1-\pi(s)$. As a consequence, the remaining type-$G$ content after moderation is $\pi(s)(1-\lambda)x$ and the remaining type-$B$ content is $(1-\pi(s))\lambda x$. The expected harm is then $(1-\pi(s))\lambda xh$.


TO BE CONTINUED


\end{document}




% \section{Liability}

% Shutting down a platform is easier said than done. There are serious Constitutional barriers to directly prohibiting a platform, including especially the First Amendment. Regulators may also not be confident in their ability to assess the overall balance of costs and benefits from the content on the platform.

% For these reasons, it is more feasible to impose liability on platforms for the bad, i.e. illegal, content that they carry. Rather than being prohbiited entirely from operating, they are penalized if and only if they host content that society has chosen to prohibit. 



% \section{Filtering}

% Even better that shutting down a platform entirely merely because it has some bad content on it would be distinguishing the good content from the bad and removing the bad content while leaving up the good.  So ideally, from society's perspective, would be for platforms to implement filters that can distinguish wheat, sheep, and babies from chaff, goats, and bathwater.

% We model filtering by giving the platform a new option. It can distinguish between good and bad content at a cost of $C > 0$ per item. It must forego the revenue $R$ from any item of content that it removes, but otherwise blocking is costless. 

% Now the platform has three options instead of two. As before, it has the option of allowing all content to stay up for a total profit of $nR - nL\beta$. As before, it has the option of shutting down for a profit of $0$. It will still choose between them according to equation~\ref{shutdown1}, shutting down if $L$ is too high.



% Note that it is never rational for the platform to pay to examine content and then leave the bad items up. In the case where $R > L$, the platform is better off not paying $C$ at all and leaving the bad items up without even looking at them. So there are three options -- no filtering, filtering, and shutdown -- and not four.


% This might suggest that society should simply increase the liability $L$
% per bad item to the point at which a platform prefers to filter. But the platform's other option also functions as a constraint. If filtering is too expensive, the platform will prefer to shut down rather than bother. This constraint kicks in unless $nR\gamma - nC > 0$, i.e. when 
% \begin{equation}
% \label{shutdown2}
% R \;>\; C\frac{1}{1 - \beta}
% \end{equation}
% In words, unless the platform's revenue for each item of good content (all that it can leave up with filtering) exceeds its expected cost of filtering per item of good content, it is better off shutting down than filtering. This time the math of low-prevalence bad items cuts the other way, albeit not as strongly. If half of the items are bad, then its revenue must be greater than $2C$ for filtering to beat shutting down. But if one millionth of all items are bad, the platform's revenue per good item must be greater than about $1.000001C$. Notice that this value approaches from above a limit of $C$ as the prevalence of bad content decreases, and can never go below $C$. In the limit, if the platform's cost to filter per item exceeds its revenue per item --- if $C > R$ --- then it is never worthwhile for the platform to filter. It is better off just shutting down.

% These two constraints together mean that a high enough level of liability can force a platform to shut down.  

% What society ought to do, then, depends on the relative magnitudes of $\gamma$, $\beta$, $R$, $C$, $G$, and $B$. Filtering is a feasible option as long as $R > C/(1 - \beta)$. If so, then any level of liability $L > R + C/\beta$ will cause the platform to shift from leaving all content up to filtering.If filtering is unprofitable for the platform, then society must choose between no filtering and shutdown per equation \ref{shutdown1}. 

%

